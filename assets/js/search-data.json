{
  
    
        "post0": {
            "title": "How I Implemented Speed Run Ethereum Challenge 1: Decentralized Staking App",
            "content": "What I‚Äôm Building . In this post, I‚Äôll show you how I implemented Challenge 1 of Speed Run ETH. . Here‚Äôs the description from the challenge page: https://speedrunethereum.com/challenge/decentralized-staking . üè¶ Build a Staker.sol contract that collects ETH from numerous addresses using a payable stake() function and keeps track of balances. After some deadline if it has at least some threshold of ETH, it sends it to an ExampleExternalContract and triggers the complete() action sending the full balance. If not enough ETH is collected, allow users to withdraw(). . The Code . Deployed contract address: https://rinkeby.etherscan.io/address/0x137cbEAA01865C1Ca907e4692bc1307c72e3b9d4 . Deployed UI: https://hard-spiders.surge.sh/ . // SPDX-License-Identifier: MIT pragma solidity 0.8.4; import &quot;hardhat/console.sol&quot;; import &quot;./ExampleExternalContract.sol&quot;; contract Staker { // VARIABLES /// External contract that will hold staked funds if threshold reached ExampleExternalContract public exampleExternalContract; /// Balances of user&#39;s staked funds mapping (address =&gt; uint256) public balances; /// Staking threshold uint256 public constant threshold = 1 ether; /// Staking deadline uint256 public deadline = block.timestamp + 72 hours; /// Boolean set if threshold is not reached by the deadline bool public openForWithdraw; // EVENTS /// Emit when stake() is called event Stake(address sender, uint256 value); // MODIFIERS /// Modifier that checks whether the required deadline has passed modifier deadlinePassed(bool requireDeadlinePassed) { uint256 timeRemaining = timeLeft(); if (requireDeadlinePassed) { require(timeRemaining &lt;= 0, &quot;Deadline has not been passed yet&quot;); } else { require(timeRemaining &gt; 0, &quot;Deadline is already passed&quot;); } _; } /// Modifier that checks whether the external contract is completed modifier stakingNotCompleted() { bool completed = exampleExternalContract.completed(); require(!completed, &quot;Staking period has completed&quot;); _; } constructor(address exampleExternalContractAddress) public { exampleExternalContract = ExampleExternalContract(exampleExternalContractAddress); } // Collect funds in a payable `stake()` function and track individual `balances` with a mapping: // ( make sure to add a `Stake(address,uint256)` event and emit it for the frontend &lt;List/&gt; display ) function stake() public payable deadlinePassed(false) stakingNotCompleted { // update the sender&#39;s balance balances[msg.sender] += msg.value; // emit Stake event to notify the UI emit Stake(msg.sender, msg.value); } // After some `deadline` allow anyone to call an `execute()` function // It should either call `exampleExternalContract.complete{value: address(this).balance}()` to send all the value function execute() public stakingNotCompleted { uint256 contractBalance = address(this).balance; if (contractBalance &gt;= threshold) { // if the `threshold` is met, send the balance to the externalContract exampleExternalContract.complete{value: contractBalance}(); } else { // if the `threshold` was not met, allow everyone to call a `withdraw()` function openForWithdraw = true; } } // Add a `withdraw(address payable)` function lets users withdraw their balance function withdraw(address payable _to) public deadlinePassed(true) stakingNotCompleted { // check the amount staked did not reach the threshold by the deadline require(openForWithdraw, &quot;Not open for withdraw&quot;); // get the sender balance uint256 userBalance = balances[msg.sender]; // check if the sender has a balance to withdraw require(userBalance &gt; 0, &quot;userBalance is 0&quot;); // reset the sender&#39;s balance balances[msg.sender] = 0; // transfer sender&#39;s balance to the `_to` address (bool sent, ) = _to.call{value: userBalance}(&quot;&quot;); // check transfer was successful require(sent, &quot;Failed to send to address&quot;); } /// Add a `timeLeft()` view function that returns the time left before the deadline for the frontend function timeLeft() public view returns (uint256) { if (block.timestamp &gt;= deadline) { return 0; } else { return deadline - block.timestamp; } } // Add the `receive()` special function that receives eth and calls stake() receive() external payable { stake(); } } . Checkpoint 2: ü•© Staking üíµ . The first challenge after getting my environment set up is to implement staking. . I add the 2 variables needed . mapping ( address =&gt; uint256 ) public balances; uint256 public constant threshold = 1 ether; . Then I implement the staking function . /// Emit when stake() is called event Stake(address sender, uint256 value); /// Modifier that checks whether the external contract is completed modifier stakingNotCompleted() { bool completed = exampleExternalContract.completed(); require(!completed, &quot;Staking period has completed&quot;); _; } /// Modifier that checks whether the required deadline has passed modifier deadlinePassed(bool requireDeadlinePassed) { uint256 timeRemaining = timeLeft(); if (requireDeadlinePassed) { require(timeRemaining &lt;= 0, &quot;Deadline has not been passed yet&quot;); } else { require(timeRemaining &gt; 0, &quot;Deadline is already passed&quot;); } _; } // Collect funds in a payable `stake()` function and track individual `balances` with a mapping: // ( make sure to add a `Stake(address,uint256)` event and emit it for the frontend &lt;List/&gt; display ) function stake() public payable deadlinePassed(false) stakingNotCompleted { // update the sender&#39;s balance balances[msg.sender] += msg.value; // emit Stake event to notify the UI emit Stake(msg.sender, msg.value); } . Since the stake function is marked as payable, the function is able to recieve ether and has access to the sender‚Äôs address through msg.sender and the amount of ether sent through msg.value. . Using these variables, I can update the contract‚Äôs balances mapping with the total amount of ether that address has sent. . You‚Äôll notice I didn‚Äôt have to initialize balances[msg.sender] to 0. Solidity will automatically initialize variables to 0 values. . After updating, the function emit‚Äôs a Stake event. This is how I alert the UI that something has happened on the blockchain. . Extra Credit - modifiers . I‚Äôve included 2 modifiers here that are useful for later challenges. I separated these specific validations because they‚Äôre needed in multiple functions as we‚Äôll see later. . The first is deadlinePassed. If I pass false to this modifer, it means the function will only execute if the deadline has NOT passed. We‚Äôll see a different usage of this modifier in the withdraw function. . There is also the stakingNotCompleted modifier. If the balance has already been sent to the external contract, staking is completed and execute will fail and revert. . Checkpoint 3: üî¨ State Machine / Timing ‚è± . The next checkout is the core logic of the contract. I need to keeps track of what state the contract is in. Is it open for staking? Does the contract balance exceed the threshold? Have we passed the deadline? . I‚Äôll walk through each function separately. . First, there‚Äôs execute . // Staking deadline uint256 public deadline = block.timestamp + 30 seconds; /// Boolean set if threshold is not reached by the deadline bool public openForWithdraw; // After some `deadline` allow anyone to call an `execute()` function // It should either call `exampleExternalContract.complete{value: address(this).balance}()` to send all the value function execute() public stakingNotCompleted { uint256 contractBalance = address(this).balance; if (contractBalance &gt;= threshold) { // if the `threshold` is met, send the balance to the externalContract exampleExternalContract.complete{value: contractBalance}(); } else { // if the `threshold` was not met, allow everyone to call a `withdraw()` function openForWithdraw = true; } } . This first checks if the deadline has passed or not through the stakingNotCompleted modifier. . If the staking is not completed, we next check the contract balance and see if it‚Äôs exceeded the threshold. If so, we complete the staking period by sending the balance to the external contract. . If not, I set the openForWithdraw boolean to true and allow everyone to withdraw their money back. . Next, let‚Äôs look at the withdraw function . // Add a `withdraw(address payable)` function lets users withdraw their balance function withdraw(address payable _to) public deadlinePassed(true) stakingNotCompleted { // check the amount staked did not reach the threshold by the deadline require(openForWithdraw, &quot;Not open for withdraw&quot;); // get the sender balance uint256 userBalance = balances[msg.sender]; // check if the sender has a balance to withdraw require(userBalance &gt; 0, &quot;userBalance is 0&quot;); // reset the sender&#39;s balance balances[msg.sender] = 0; // transfer sender&#39;s balance to the `_to` address (bool sent, ) = _to.call{value: userBalance}(&quot;&quot;); // check transfer was successful require(sent, &quot;Failed to send to address&quot;); } . Once the modifiers are passed, I check that the contract is open for withdraw and that the user has ether to send back. Before actually sending the ether, notice that I set the sender‚Äôs balance to 0 in the contract‚Äôs balances mapping. . I always want to make all state changes before calling other contracts or sending ether. This prevents something known as a Re-Entrancy attack. If we were to send the balance before setting the sender‚Äôs balance to 0, it‚Äôs possible for the sender to call withdraw multiple times before the _to.call function completes. The contract will think the sender still has an ether balance and will keep sending extra ether until the first call completes. . Finally, after all checks have passed, I send the ether and check that the transfer was successful. . The last function to mention is the helper function timeLeft . /// Add a `timeLeft()` view function that returns the time left before the deadline for the frontend function timeLeft() public view returns (uint256) { if (block.timestamp &gt;= deadline) { return 0; } else { return deadline - block.timestamp; } } . This is used in deadlinePassed to see how much time is left in the staking period. . Checkpoint 4: üíµ Receive Function / UX üôé . For this checkpoint, we implement a receive function to automatically stake eth which has been sent to the contract address. . // Add the `receive()` special function that receives eth and calls stake() receive() external payable { stake(); } . So What Did I Do? . I updated the code to point at the Rinkeby ethereum test net and deployed the frontend to https://hard-spiders.surge.sh/ . You can try it out for yourself although the staking period is over by now. . Now on to Challenge 2: Token Vendor. . See you then! .",
            "url": "https://leogau.dev/2022/02/07/how-i-implemented-speed-run-ethereum-challenge-1-decentralized-staking-app.html",
            "relUrl": "/2022/02/07/how-i-implemented-speed-run-ethereum-challenge-1-decentralized-staking-app.html",
            "date": " ‚Ä¢ Feb 7, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Identify MNIST Digits With 97% Accuracy By Using A 2 Layer Neural Network",
            "content": "What I&#39;m Building . In this post, I&#39;ll show you how I implemented a 2 layer neural network which is able to achieve over 97% accuracy on the MNIST test set. . This network builds on the work in my previous posts. If you&#39;d like a refresher, they are: . How I Implemented The Most Simple Neural Network Using Python | How I Identify Handwritten Digits Using Only Python | How I Go From 70 Lines Of Code To Only 26 Using The NumPy Library | . Now, I&#39;ll show you the code first and then explain some concepts you need to understand what&#39;s going on. . The Code . import numpy as np np.random.seed(1) def relu(x): return (x &gt; 0) * x def relu2deriv(output): return output &gt; 0 def flatten_image(image): return np.array(image).reshape(1, 28*28) class NeuralNet: def __init__(self): self.alpha = 0.00001 self.hidden_size = 1000 self.weights_0_1 = np.random.random((28 * 28, self.hidden_size)) * 0.0001 self.weights_1_2 = np.random.random((self.hidden_size, 10)) * 0.0001 def predict(self, input): layer_0 = input layer_1 = relu(np.dot(layer_0, self.weights_0_1)) layer_2 = np.dot(layer_1, self.weights_1_2) return layer_2 def train(self, input, labels, epochs): for i in range(epochs): layer_2_error = 0 for j in range(len(input)): layer_0 = input[j] layer_1 = relu(np.dot(layer_0, self.weights_0_1)) layer_2 = np.dot(layer_1, self.weights_1_2) label = labels[j] goal = np.zeros(10) goal[label] = 1 layer_2_error = np.sum((layer_2 - goal) ** 2) layer_2_delta = (layer_2 - goal) layer_1_delta = layer_2_delta.dot(self.weights_1_2.T) * relu2deriv(layer_1) self.weights_1_2 = self.weights_1_2 - (self.alpha * layer_1.T.dot(layer_2_delta)) self.weights_0_1 = self.weights_0_1 - (self.alpha * layer_0.T.dot(layer_1_delta)) print(&quot;Error: &quot; + str(layer_2_error)) . from keras.datasets import mnist (x_train, y_train), (x_test, y_test) = mnist.load_data() images = x_train labels = y_train prepared_images = [flatten_image(image) for image in images] prepared_labels = np.array(labels) nn = NeuralNet() nn.train(prepared_images, prepared_labels, 5) test_set = x_test test_labels = y_test num_correct = 0 for i in range(len(test_set)): prediction = nn.predict(flatten_image(test_set[i])) correct = test_labels[i] if np.argmax(prediction) == int(correct): num_correct += 1 print(str(num_correct/len(test_set) * 100) + &quot;%&quot;) . Why Do I Need More Than One Layer? . At their core, neural networks find correlations between the input data and target data. Sometimes there&#39;s just no correlation to be found using the number of weights given. . One way to increase the accuracy is to give the network more weights to use. And the way to give it more weights is to add more layers. . We might not have correlation between the input and output layers but we can create an extra layer to help us out. . Now, even with more weights, there&#39;s another problem. For any 3 layer network, there is a 2 layer network which can do that exact same thing. . Since there is no special processing at the extra layer, it‚Äôs not contributing any new information to the network. It correlates 1:1 with the input layer. What we need is for the middle layer to sometime correlate and sometimes not correlate with the input layer. We need it to have it‚Äôs own processing. . This is called conditional correlation. One way to create conditional correlation is to turn off the node when the value would be negative. If the value is negative, it would normally be negatively correlated with the input. However, if we turn it off (set the value to 0) then it doesn‚Äôt effect the output at all. . This means a node can be selectively correlated with inputs. . Let me flesh this out with an example. . Let‚Äôs say a node has 2 inputs, left and right. The left input is 1 and the right input is -1. If we use both weights, the node would be 0. They cancel each other out. However, if we set right input to 0, then the node would only be correlated with the left input value. The node is now adding additional information to the network by saying, ‚ÄúMake me perfectly correlated with the left input, but only if the right input is 0.‚Äù . This wasn‚Äôt possible earlier. This is the power of adding layers to the network. . The technical term for a situation where 2 variables are not predictable from a straight line is ‚Äúnonlinearity‚Äù. The functions we use to create nonlinearities are called activation functions. The one I use in my network - turn off the node when it&#39;s value would be negative - is called Rectified Linear Unit (ReLU). . So that&#39;s one piece of the puzzle. I add another layer to give the neural network more weights to play with and create conditional correlation with activation functions. . Another problem you might be thinking about now is, how do we adjust the weights of the new layers? In a single layer network, we get the derivative of the delta and the input, but now we have weights that didn‚Äôt directly contribute to the loss. . Backpropagation . The process of updating the weights in intermediate layers is called backpropagation. . How do you use the delta from the final layer (layer_2) to figure out how to change the weights in an intermediate layer (layer_1)? It turns out, through some calculus, I can multiply the layer_2 delta with the layer_1 inputs. . If I had more layers, I could keep multiplying the delta with the node input to get the weight_deltas. . With knowledge of activation functions and backpropagation, I can now break down what I did in code. . Import Libraries And Get Data . For an explaination of this code, look at some of my previous posts. . import numpy as np np.random.seed(1) . from keras.datasets import mnist (x_train, y_train), (x_test, y_test) = mnist.load_data() images = x_train labels = y_train . Activation Functions . Like I mentioned above, the activation function I used is called ReLU. ReLU will set any values that would be negative to 0. Any positive values remain as is. . def relu(x): return (x &gt; 0) * x . During backpropagation, I don‚Äôt want to adjust the weights if ReLU set it to 0. Therefore, I need a function to tell me if ReLu did that or not. . The relu2deriv function will be used to cancel the weight adjustment if it was altered during the prediction time. If ReLU set the value to 0, the weight should not be adjusted at all. . def relu2deriv(output): return output &gt; 0 . Finally, I have my trusty flatten_image function to prepare the data. . def flatten_image(image): return np.array(image).reshape(1, 28*28) . The NeuralNet class has three new features to look at. . First, it has 2 sets of weights and a hidden_size which determines the size of the hidden layer. . self.hidden_size = 1000 self.weights_0_1 = np.random.random((28 * 28, self.hidden_size)) * 0.0001 self.weights_1_2 = np.random.random((self.hidden_size, 10)) * 0.0001 . A hidden_size=1000 is going to give the network a lot more weights to figure out the correlation between the images and labels. . Second, the prediction function now does 2 weighted sums . layer_0 = input layer_1 = relu(np.dot(layer_0, self.weights_0_1)) layer_2 = np.dot(layer_1, self.weights_1_2) . After setting layer_0 as the inputs, I calculate layer_1 by taking the weighted sum of layer_0 and the first set of weights, self.weights_0_1. I then use our activation function on the result to get layer_1. The final layer, layer_2, is the weighted sum of layer_1 and the second set of weights, self.weights_1_2. . Finally, let&#39;s look at how the weights get updated. . layer_2_delta = (layer_2 - goal) layer_1_delta = layer_2_delta.dot(self.weights_1_2.T) * relu2deriv(layer_1) layer_2_weight_delta = layer_1.T.dot(layer_2_delta) layer_1_weight_delta = layer_0.T.dot(layer_1_delta) self.weights_1_2 = self.weights_1_2 - (self.alpha * layer_2_weight_delta) self.weights_0_1 = self.weights_0_1 - (self.alpha * layer_1_weight_delta) . I get the layer_2_delta like before, getting the difference between the prediction and the goal. The layer_1_delta is derived by taking the weighted sum between the layer_2_delta and the weights connected to that layer, self.weights_1_2. I also need to use the relu2deriv function at this point to tell me if ReLU adjusted the node values or not. . A note on the .T syntax. .T in NumPy is shorthand for transpose. It lets me reshape the matrix so everything lines up correctly to do matrix math. . So that&#39;s the deltas, but how much do I adjust the weights by? . I find the layer_2_weight_delta but calculating the weighted sum of layer_1 and layer_2_delta. So it&#39;s the inputs into the layer and the delta at that layer. The layer_1_weight_delta is the same. I calculate the weighted sum of the inputs into that layer, layer_0, and the delta, layer_1_delta. . The weights are still adjusted by subtracting the weight deltas multiplied by some learning rate (self.alpha). . class NeuralNet: def __init__(self): self.alpha = 0.00001 self.hidden_size = 1000 self.weights_0_1 = np.random.random((28 * 28, self.hidden_size)) * 0.0001 self.weights_1_2 = np.random.random((self.hidden_size, 10)) * 0.0001 def predict(self, input): layer_0 = input layer_1 = relu(np.dot(layer_0, self.weights_0_1)) layer_2 = np.dot(layer_1, self.weights_1_2) return layer_2 def train(self, input, labels, epochs): for i in range(epochs): layer_2_error = 0 for j in range(len(input)): layer_0 = input[j] layer_1 = relu(np.dot(layer_0, self.weights_0_1)) layer_2 = np.dot(layer_1, self.weights_1_2) label = labels[j] goal = np.zeros(10) goal[label] = 1 layer_2_error = np.sum((layer_2 - goal) ** 2) layer_2_delta = (layer_2 - goal) layer_1_delta = layer_2_delta.dot(self.weights_1_2.T) * relu2deriv(layer_1) layer_2_weight_delta = layer_1.T.dot(layer_2_delta) layer_1_weight_delta = layer_0.T.dot(layer_1_delta) self.weights_1_2 = self.weights_1_2 - (self.alpha * layer_2_weight_delta) self.weights_0_1 = self.weights_0_1 - (self.alpha * layer_1_weight_delta) print(&quot;Error: &quot; + str(layer_2_error)) . prepared_images = [flatten_image(image) for image in images] prepared_labels = np.array(labels) nn = NeuralNet() nn.train(prepared_images, prepared_labels, 5) test_set = x_test test_labels = y_test num_correct = 0 for i in range(len(test_set)): prediction = nn.predict(flatten_image(test_set[i])) correct = test_labels[i] if np.argmax(prediction) == int(correct): num_correct += 1 print(str(num_correct/len(test_set) * 100) + &quot;%&quot;) . Error: 0.10415136838556452 Error: 0.07391762691913144 Error: 0.0634422993538635 Error: 0.05133955942375137 Error: 0.039768839003841615 97.48% . If I train this network over 5 epochs like the other networks, I see the error go down to 0.03 and I get 97.48% accuracy. . Not too shabby! And what an improvement over the 1 layer network which only got 76% correct! . What&#39;s Next? . Next I&#39;ll dive into regularization and try to increase the accuracy even further. . See you then! . Find me on Twitter if you want discuss any of what I&#39;ve written! .",
            "url": "https://leogau.dev/2021/03/16/MNIST-Deep-Neural-Network.html",
            "relUrl": "/2021/03/16/MNIST-Deep-Neural-Network.html",
            "date": " ‚Ä¢ Mar 16, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "How I Go From 70 Lines Of Code To Only 26 Using The NumPy Library",
            "content": "Context . In my previous post I implemented a nerual network to understand handwritten digits using matrix math functions I implemented myself. . Luckily, I don&#39;t have to be doing this going forward in the future. The NumPy library will do this all for me. Numpy is the one of the foundational libraries in the Python scientific computing ecosystem. It provides a high-performance, multidimentional array object which makes it fast and easy to work with matrices. . In this post, I&#39;ll show you how I use NumPy to replace the hand written math functions I wrote. . The Original Code Using Only Python . Below is the original code with my owm matrix multiplication functions. . def flatten_image(image): return list(itertools.chain.from_iterable(image)) def weighted_sum(a, b): assert(len(a) == len(b)) output = 0 for i in range(len(a)): output += (a[i] * b[i]) return output def vector_matrix_multiplication(a, b): output = [0 for i in range(10)] for i in range(len(output)): assert(len(a) == len(b[i])) output[i] = weighted_sum(a, b[i]) return output def zeros_matrix(rows, cols): output = [] for r in range(rows): output.append([0 for col in range(cols)]) return output def outer_product(a, b): output = zeros_matrix(len(a), len(b)) for i in range(len(a)): for j in range(len(b)): output[i][j] = a[i] * b[j] return output class NeuralNet: def __init__(self): self.weights = [ [0.0000 for i in range(784)], [0.0001 for i in range(784)], [0.0002 for i in range(784)], [0.0003 for i in range(784)], [0.0004 for i in range(784)], [0.0005 for i in range(784)], [0.0006 for i in range(784)], [0.0007 for i in range(784)], [0.0008 for i in range(784)], [0.0009 for i in range(784)] ] self.alpha = 0.0000001 def predict(self, input): return vector_matrix_multiplication(input, self.weights) def train(self, input, labels, epochs): for i in range(epochs): for j in range(len(input)): pred = self.predict(input[j]) label = labels[j] goal = [0 for k in range(10)] goal[label] = 1 error = [0 for k in range(10)] delta = [0 for k in range(10)] for a in range(len(goal)): delta[a] = pred[a] - goal[a] error[a] = delta[a] ** 2 weight_deltas = outer_product(delta, input[j]) for x in range(len(self.weights)): for y in range(len(self.weights[0])): self.weights[x][y] -= (self.alpha * weight_deltas[x][y]) . Implement The Helper Functions With NumPy . To help me better understand how NumPy slots into this code, I&#39;m going to keep my helper functions but implement them using NumPy. For example, I still have a weighted sum function but instead of hand calculating the weighted sum, I use the NumPy dot function. . def flatten_image(image): return image.reshape(1, 28*28) def weighted_sum(a, b): return a.dot(b) def vector_matrix_multiplication(a, b): return np.matmul(input, weights.T) def zeros_matrix(rows, cols): return np.zeros((rows, cols)) def outer_product(a, b): return np.outer(a, b) class NeuralNet: def __init__(self): self.weights = np.random.random((10, 28 * 28)) * 0.0001 self.alpha = 0.0000001 def predict(self, input): return vector_matrix_multiplication(input, self.weights) def train(self, input, labels, epochs): for i in range(epochs): for j in range(len(input)): pred = self.predict(input[j]) label = labels[j] goal = np.zeros(10) goal[label] = 1 delta = pred - goal error = delta ** 2 weight_deltas = outer_product(delta, input[j]) self.weights -= (self.alpha * weight_deltas) . Use The NumPy Functions Inline . Already you can see the code is a lot cleaner. . If we remove those helper functions and do everything inline, the code shrinks even more. The original implmentation is 70 lines long and this one is only 26 lines. . The NumPy library is doing a lot of work for me. . def flatten_image(image): return image.reshape(1, 28*28) class NeuralNet: def __init__(self): self.weights = np.random.random((10, 28 * 28)) * 0.0001 self.alpha = 0.0000001 def predict(self, input): return np.matmul(input, self.weights.T) def train(self, input, labels, epochs): for i in range(epochs): for j in range(len(input)): pred = self.predict(input[j]) label = labels[j] goal = np.zeros(10) goal[label] = 1 delta = pred - goal error = delta ** 2 weight_deltas = np.outer(delta, input[j]) self.weights -= (self.alpha * weight_deltas) . import itertools import numpy as np from keras.datasets import mnist (x_train, y_train), (x_test, y_test) = mnist.load_data() images = x_train labels = y_train prepared_images = [flatten_image(image) for image in images] prepared_labels = np.array(labels) nn = NeuralNet() nn.train(prepared_images, prepared_labels, 5) test_set = x_test test_labels = y_test num_correct = 0 for i in range(len(test_set)): prediction = nn.predict(flatten_image(test_set[i])) correct = test_labels[i] if np.argmax(prediction) == int(correct): num_correct += 1 print(str(num_correct/len(test_set) * 100) + &quot;%&quot;) . 76.05% . So What&#39;s Next? . The code is now so efficient I can train on the full dataset and the accuracy gets a little bump. . In the next post we&#39;ll see if adding a layer helps improve this accuracy even more. .",
            "url": "https://leogau.dev/2021/03/10/MNIST-Numpy.html",
            "relUrl": "/2021/03/10/MNIST-Numpy.html",
            "date": " ‚Ä¢ Mar 10, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "How I Identify Handwritten Digits Using Only Python",
            "content": "What I&#39;m Building . In this post I&#39;ll show you how I built a neural network which takes an array of numbers representing a handwritten digit and output a prediction of what digit it is. . The handwritten digits are from the famous MNIST dataset. The Modified National Institute of Standards and Technology (MNIST) dataset is a collection of 60,000 small, square 28√ó28 pixel grayscale images of handwritten single digits between 0 and 9. . The task is to classify a given image into one of the 10 digits. . I‚Äôm doing it all in Python. . Let&#39;s get started. . The Code . import itertools from keras.datasets import mnist (x_train, y_train), (x_test, y_test) = mnist.load_data() images = x_train[0:1000] labels = y_train[0:1000] def flatten_image(image): return list(itertools.chain.from_iterable(image)) def weighted_sum(a, b): assert(len(a) == len(b)) output = 0 for i in range(len(a)): output += (a[i] * b[i]) return output def vector_matrix_multiplication(a, b): output = [0 for i in range(10)] for i in range(len(output)): assert(len(a) == len(b[i])) output[i] = weighted_sum(a, b[i]) return output def zeros_matrix(rows, cols): output = [] for r in range(rows): output.append([0 for col in range(cols)]) return output def outer_product(a, b): output = zeros_matrix(len(a), len(b)) for i in range(len(a)): for j in range(len(b)): output[i][j] = a[i] * b[j] return output class NeuralNet: def __init__(self): self.weights = [ [0.0000 for i in range(784)], [0.0001 for i in range(784)], [0.0002 for i in range(784)], [0.0003 for i in range(784)], [0.0004 for i in range(784)], [0.0005 for i in range(784)], [0.0006 for i in range(784)], [0.0007 for i in range(784)], [0.0008 for i in range(784)], [0.0009 for i in range(784)] ] self.alpha = 0.0000001 def predict(self, input): return vector_matrix_multiplication(input, self.weights) def train(self, input, labels, epochs): for i in range(epochs): for j in range(len(input)): pred = self.predict(input[j]) label = labels[j] goal = [0 for k in range(10)] goal[label] = 1 error = [0 for k in range(10)] delta = [0 for k in range(10)] for a in range(len(goal)): delta[a] = pred[a] - goal[a] error[a] = delta[a] ** 2 weight_deltas = outer_product(delta, input[j]) for x in range(len(self.weights)): for y in range(len(self.weights[0])): self.weights[x][y] -= (self.alpha * weight_deltas[x][y]) # Train on first image first_image = images[0] first_label = labels[0] input = [flatten_image(first_image)] label = [first_label] nn = NeuralNet() nn.train(input, label, 5) prediction = nn.predict(input[0]) print(prediction) print(&quot;The label is: &quot; + str(label[0]) + &quot;. The prediction is: &quot; + str(prediction.index(max(prediction)))) # Train on full dataset prepared_images = [flatten_image(image) for image in images] mm = NeuralNet() mm.train(prepared_images, labels, 45) # Test 1 prediction prediction = mm.predict(prepared_images[3]) print(&quot;That image is the number &quot; + str(prediction.index(max(prediction)))) # Calculate accuracy test_set = x_test[0:100] test_labels = y_test[0:100] num_correct = 0 for i in range(len(test_set)): prediction = mm.predict(flatten_image(test_set[i])) correct = test_labels[i] if prediction.index(max(prediction)) == int(correct): num_correct += 1 print(str(num_correct/len(test_set) * 100) + &quot;%&quot;) . Get Dataset . The keras library helpfully includes the dataset so I can import it from the library. . from keras.datasets import mnist (x_train, y_train), (x_test, y_test) = mnist.load_data() images = x_train[0:1000] labels = y_train[0:1000] . When I call load_data(), I get back two tuples: a training set and a test set. To successfully finishing training on my personal laptop, I had to limit the data to the first 1000 elements. When I tried training on the full data set, it was hadn&#39;t finished after a full 24 hours and I had to kill the process to use my laptop :D. . With only 1000 images, the best accuracy I achieved was about 75%. Maybe you can tweak the numbers and get something better! . Getting back to the data, if I take a look at one of the images in the training set, I see that it is an array of arrays - a matrix. The numbers range from 0 to 255 - each representing the greyscale value of the pixel at a particular position in the image. . images[0] . array([[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 18, 18, 18, 126, 136, 175, 26, 166, 255, 247, 127, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 30, 36, 94, 154, 170, 253, 253, 253, 253, 253, 225, 172, 253, 242, 195, 64, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 49, 238, 253, 253, 253, 253, 253, 253, 253, 253, 251, 93, 82, 82, 56, 39, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 18, 219, 253, 253, 253, 253, 253, 198, 182, 247, 241, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 80, 156, 107, 253, 253, 205, 11, 0, 43, 154, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 14, 1, 154, 253, 90, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 139, 253, 190, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 190, 253, 70, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 35, 241, 225, 160, 108, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 81, 240, 253, 253, 119, 25, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 45, 186, 253, 253, 150, 27, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 93, 252, 253, 187, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 249, 253, 249, 64, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 46, 130, 183, 253, 253, 207, 2, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 39, 148, 229, 253, 253, 253, 250, 182, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 24, 114, 221, 253, 253, 253, 253, 201, 78, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 23, 66, 213, 253, 253, 253, 253, 198, 81, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 18, 171, 219, 253, 253, 253, 253, 195, 80, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 55, 172, 226, 253, 253, 253, 253, 244, 133, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 136, 253, 253, 253, 212, 135, 132, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8) . If I look at the first label, I see the number five. This means that the collection of numbers in images[0] represents is the number 5. . labels[0] . 5 . Prepare Data . The matrix math that I implement does not know how to handle an array of arrays so, the first thing I do is prepare the data by flattening the image into a single array. . import itertools def flatten_image(image): return list(itertools.chain.from_iterable(image)) . What I&#39;m doing in this function is using the itertools library to flatten the array. Specifically, I&#39;m using the .chain.from_iterable() method to give me one element at a time. Then I use the list() function to create a flat list to return. . When I print the first image, I see that all the numbers are in one flat array. . print(flatten_image(images[0])) . [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 18, 18, 18, 126, 136, 175, 26, 166, 255, 247, 127, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 30, 36, 94, 154, 170, 253, 253, 253, 253, 253, 225, 172, 253, 242, 195, 64, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 49, 238, 253, 253, 253, 253, 253, 253, 253, 253, 251, 93, 82, 82, 56, 39, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 18, 219, 253, 253, 253, 253, 253, 198, 182, 247, 241, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 80, 156, 107, 253, 253, 205, 11, 0, 43, 154, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 14, 1, 154, 253, 90, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 139, 253, 190, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 190, 253, 70, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 35, 241, 225, 160, 108, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 81, 240, 253, 253, 119, 25, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 45, 186, 253, 253, 150, 27, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 93, 252, 253, 187, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 249, 253, 249, 64, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 46, 130, 183, 253, 253, 207, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 39, 148, 229, 253, 253, 253, 250, 182, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 24, 114, 221, 253, 253, 253, 253, 201, 78, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 23, 66, 213, 253, 253, 253, 253, 198, 81, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 18, 171, 219, 253, 253, 253, 253, 195, 80, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 55, 172, 226, 253, 253, 253, 253, 244, 133, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 136, 253, 253, 253, 212, 135, 132, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] . Matrix Math Helper Functions . Now that I&#39;ve prepared the data, I can move on to the next step - implement matrix math. . Since I&#39;m working with arrays, I need math functions which understand arrays. You may remember from the previous post that a neural network makes predictions by multiplying the input by the weights. So one thing I need to do now is figure out how to do matrix multiplication. . In order to do matrix multipliation, I need a method to calculate weighted sums. . def weighted_sum(a, b): assert(len(a) == len(b)) output = 0 for i in range(len(a)): output += (a[i] * b[i]) return output . The weighted sum function takes two arrays of the same length. It multiplies each number in the same index and adds the result to a running sum. So the weighted sum takes two arrays and gives you back a single number. . The best way to think about what this single number represents is as a score of similarity between two arrays. The higher the weighted sum, the more similar arrays a and b are to each other. Roughly speaking, the neural network will give higher scores to inputs that are more similar to its weights. . def vector_matrix_multiplication(a, b): output = [0 for i in range(10)] for i in range(len(output)): assert(len(a) == len(b[i])) output[i] = weighted_sum(a, b[i]) return output . Next, I have the matrix multiplication method. This calculates the weighted sum between weight and input for each position in the array. When it&#39;s done, I get an array of weighted sums. . In my case, the returned output of 10 elements contain the probability of which digit the input represents. Whichever index has the highest number is the prediction for what digit is in the image. . I need two other matrix math helpers. These functions will be used to adjust the weights in the right direction. . First, I have a zeros matrix method which creates a matrix filled with zeros. . def zeros_matrix(rows, cols): output = [] for r in range(rows): output.append([0 for col in range(cols)]) return output . This is used to implement a function to calculate the outer product of two matrices. . The outer product does an elementwise multiplication between two matricies. This will be used to tell the neural network how to change its weights. . def outer_product(a, b): output = zeros_matrix(len(a), len(b)) for i in range(len(a)): for j in range(len(b)): output[i][j] = a[i] * b[j] return output . Okay. That&#39;s a lot of math. Let&#39;s find out how these functions are being used in the neural network. . Neural Network . class NeuralNet: def __init__(self): self.weights = [ [0.0000 for i in range(784)], [0.0001 for i in range(784)], [0.0002 for i in range(784)], [0.0003 for i in range(784)], [0.0004 for i in range(784)], [0.0005 for i in range(784)], [0.0006 for i in range(784)], [0.0007 for i in range(784)], [0.0008 for i in range(784)], [0.0009 for i in range(784)] ] self.alpha = 0.0000001 def predict(self, input): return vector_matrix_multiplication(input, self.weights) def train(self, input, labels, epochs): for i in range(epochs): for j in range(len(input)): pred = self.predict(input[j]) label = labels[j] goal = [0 for k in range(10)] goal[label] = 1 error = [0 for k in range(10)] delta = [0 for k in range(10)] for a in range(len(goal)): delta[a] = pred[a] - goal[a] error[a] = delta[a] ** 2 weight_deltas = outer_product(delta, input[j]) for x in range(len(self.weights)): for y in range(len(self.weights[0])): self.weights[x][y] -= (self.alpha * weight_deltas[x][y]) . This neural network is similar to the one from the previous post. The only real difference is that we&#39;re using an array of numbers instead of a single number. . In the initializer, I have the weights and the alpha. I&#39;ve initialized each weight array to have 784 elements of an initial number. 784 is the number of pixels in the image. . def __init__(self): self.weights = [ [0.0000 for i in range(784)], [0.0001 for i in range(784)], [0.0002 for i in range(784)], [0.0003 for i in range(784)], [0.0004 for i in range(784)], [0.0005 for i in range(784)], [0.0006 for i in range(784)], [0.0007 for i in range(784)], [0.0008 for i in range(784)], [0.0009 for i in range(784)] ] self.alpha = 0.0000001 . The prediction function is again multplying the input by the weights. . def predict(self, input): return vector_matrix_multiplication(input, self.weights) . The training function iterates through the dataset an epoch number of times. . for i in range(epochs): for j in range(len(input)): . For each image, it makes a prediction . pred = self.predict(input[j]) . Next we transform the label into a format that the neural network expects. . label = labels[j] goal = [0 for k in range(10)] goal[label] = 1 . I create an array of ten 0s and then set the index of the goal prediction to 1. So all the wrong answers are 0 and the right answer is 1. . Next, I calculate the error and the delta. . error = [0 for k in range(10)] delta = [0 for k in range(10)] for a in range(len(goal)): delta[a] = pred[a] - goal[a] error[a] = delta[a] ** 2 . I then calculate the weight deltas by using an outer product between delta and the input. . weight_deltas = outer_product(delta, input[j]) . Finally I update all the weights using the weight deltas. . for x in range(len(self.weights)): for y in range(len(self.weights[0])): self.weights[x][y] -= (self.alpha * weight_deltas[x][y]) . The main takeaway here is that this is exactly like the neural network with one digit. The only difference is that the math is done on arrays instead of on single numbers. . Training The Network On The First Data Point . Let&#39;s put this new network into action. To test it out, I take take the first image and the first label. I create a neural network and train it on that first image and label for five epochs. When I predict the digit on that same image, I see the output array is an array of 10 numbers. . first_image = images[0] first_label = labels[0] input = [flatten_image(first_image)] label = [first_label] nn = NeuralNet() nn.train(input, label, 5) prediction = nn.predict(input[0]) print(prediction) print(&quot;The label is: &quot; + str(label[0]) + &quot;. The prediction is: &quot; + str(prediction.index(max(prediction)))) . [0.0, 0.03036370905054081, 0.06072741810108162, 0.09109112715162263, 0.12145483620216324, 1.1407872249800253, 0.18218225430324525, 0.21254596335378556, 0.24290967240432648, 0.2732733814548679] The label is: 5. The prediction is: 5 . The number in index five is the greatest, so the network correctly identified the handwritten number of the number 5. . It works on one data point but what about the entire data set? . Let&#39;s do that next. . Training The Network On All The Whole Dataset . I prepare the images by flattening every image in our data set. Again, this is the first 1000 from the MNIST dataset. I create the neural network, giving it the prepared images and labels. . I run it for 5 epochs. Through trial and error I found that 5 epochs gives me the highest accuracy of just under 75%. . When it&#39;s finished, I test the network by making a prediction on a random image. It correctly identified the image. . prepared_images = [flatten_image(image) for image in images] mm = NeuralNet() mm.train(prepared_images, labels, 5) prediction = mm.predict(prepared_images[3]) print(&quot;That image is the number &quot; + str(prediction.index(max(prediction)))) . That image is the number 1 . labels[3] . 1 . To test the true accuracy, I use the test data and labels. . I run through a loop of the test set, make a prediction, checking its accuracy, and counting the number correct. . test_set = x_test test_labels = y_test num_correct = 0 for i in range(len(test_set)): prediction = mm.predict(flatten_image(test_set[i])) correct = test_labels[i] if prediction.index(max(prediction)) == int(correct): num_correct += 1 print(str(num_correct/len(test_set) * 100) + &quot;%&quot;) . 74.47% . In the end, I&#39;m able to correctly predict 3 out of every 4 images in the test set. . So What Did We Do? . This was a fun little exercise to see how neural networks use matrix math to make predictions. . What&#39;s Next? . In the next post, I‚Äôll experiment with adding multiple layers to make the network &quot;deep&quot;. I&#39;ll also swap my handwritten matrix math functions for NumPy functions and see how much easier it makes some of this for me. . See you next time! .",
            "url": "https://leogau.dev/2021/02/28/MNIST.html",
            "relUrl": "/2021/02/28/MNIST.html",
            "date": " ‚Ä¢ Feb 28, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "How I Implemented The Most Simple Neural Network Using Python",
            "content": "Context . I&#39;ve been reading the book Grokking Deep Learning by Andrew W. Trask and instead of summarizing concepts, I want to review them by building a simple neural network. This neural network will use the concepts in the first 4 chapters of the book. . What I&#39;m Building . I&#39;m going to build a neural network which outputs a target number given a specific input number. For example, given the number 5, I want the neural network to output the number 42. . Now I can hear you think to yourself, &quot;That&#39;s stupid. How is that better than a function with the line return 42 in the body?&quot; . What&#39;s cool about this code is that I didn&#39;t type the number 5 or 42 anywhere in the body of the network. Instead, I told the network I wanted it to print 42 when it received 5 as an input and it figure out how to adjust itself to do that. . In fact, I could train the network on any 2 numbers using the same code. Try changing the parameters yourself and test it out! . With that context, let&#39;s see what the code looks like for this most simple neural network. . The Code . class SimpleNN: def __init__(self): self.weight = 1.0 self.alpha = 0.01 def train(self, input, goal, epochs): for i in range(epochs): pred = input * self.weight delta = pred - goal error = delta ** 2 derivative = delta * input self.weight = self.weight - (self.alpha * derivative) print(&quot;Error: &quot; + str(error)) def predict(self, input): return input * self.weight . neural_network = SimpleNN() # Train the SimpleNN neural_network.train(input=5, goal=42, epochs=20) . Error: 1369.0 Error: 770.0625 Error: 433.16015625 Error: 243.6525878906251 Error: 137.05458068847665 Error: 77.09320163726807 Error: 43.36492592096329 Error: 24.39277083054185 Error: 13.72093359217979 Error: 7.718025145601132 Error: 4.341389144400637 Error: 2.442031393725358 Error: 1.373642658970514 Error: 0.7726739956709141 Error: 0.43462912256489855 Error: 0.24447888144275018 Error: 0.13751937081154697 Error: 0.07735464608149517 Error: 0.043511988420844 Error: 0.02447549348672308 . neural_network.predict(5) . 41.88266515825944 . After 20 rounds of training, the network&#39;s final prediction is off by about 0.02. Not bad! . Even in this barebones neural network, there&#39;s a lot going on. Let&#39;s take it line by line. . Neural Networks . A neural network is a collection of weights being used to compute an error function. That&#39;s it. . The interesting thing about this statement is that for any error function, no matter how complicated, you can compute the relationship between a weight and the final error of the network. Therefore, after each prediction, we can change each weight in the network to inch the final error towards 0. . Let&#39;s take a look what a neural network needs to make a prediction. . The 2 Things A Neural Network Needs To Make A Prediction . The Weight . self.weight = 1.0 . I mentioned before that a neural network is just &quot;a collection of weights&quot;. So what are weights? . weight is a number that the neural network stores and remembers. It can be thought of of the &quot;memory&quot; of the network. After each round of training, the network updates the weight to make more accurate predictions. . In our network, I set weight=1.0. I just used trial-and-error to figure out a good starting weight for this problem. . The Input . def train(self, input, goal, epochs): def predict(self, input): . input is a number that the neural network accepts. This can be thought of as information from the outside world. . In our network, I set input=5 when I start training the network. . So how does this thing learn? . I use a method called Stochasitc Gradient Descent to get SimpleNN to learn the training data. . At a high level, the 4 step process is: . Make a prediction using a given input | Calculate the error | Calculate the derivative to tell us how much to adjust the weights by | Adjust the weight and go back to step 1. | 1. The Prediction . pred = input * self.weight . When the neural network has both an input and weight, it multiplies them together to make a prediction. Every single neural network, from the most simple to ones with 1000s of layers works this way. . 2. How much are we off by? . delta = pred - goal error = delta ** 2 . So we&#39;ve seen that the network make a prediction by multiplying input and weight. After it makes a prediction, the network is able to calculate how much it was off by. . A neural network learning is all about error attribution. How much did each weight contribute to the overall error of the system and how can we change the weight so that error is minimized? In our example, it&#39;s easy to figure out since there is only 1 weight. . How do we calculate the error? One thing we need to keep in mind is that we want the error to be a positive number. If the error is allowed to be negative, multiple errors might accidentally cancel each other out when averaged together. . In our case, we square the amount we are off by. Why square instead of something straightforward like absolute value? Squaring gives us a sense of importance. Large errors are magnified while small errors are minimized. Therefore, we can prioritize large errors before small errors. Absolute value doesn&#39;t give us this additional sense of importance. . 3. Adjusting the weights . derivative = delta * input self.weight = self.weight - (self.alpha * derivative) . The network figures out how much to adjust the weights by using a derivative. How does derivative play into this process? What a derivative tells us is the direction and amount one variable changes when you change a different variable. In our case, derivatives tell us much much error changes when you change the weight. Given that we want error to be 0, this is exactly what we need. . The network calculates the derivative by multiplying the delta by the weight&#39;s input to get the weight_delta. weight_delta is the direction and the amount we&#39;re going to change the weight by. . self.alpha = 0.01 . One bit of nuance is the variable alpha. alpha is a throttle limiting how much we actually adjust the weights. Determining the appropriate rate of change for the weights of a neural network is a challenge. If the steps are too large, the network will overshoot the error getting to zero and start acting in unpredictable ways. If the steps are too small, the network will take a long time and need a very large number of training cycles. . The solution to this problem is to multiply partial derivative by a single number between 0 and 1. This lets us control the rate of change and adjust the learning as needed. . Finding the appropriate alpha is often done through trial and error so we&#39;re just going to hard code is here. . 4. Training rounds . neural_network.train(input=5, goal=42, epochs=20) for i in range(epochs): . Finally, there&#39;s the concept of epochs. This refers to the number of times the network will go through the entire data set. The appropriate number of epochs for a problem will often be found through trial and error. . I&#39;m using 20 in the example, which I found by running the training with different epochs and picking the lowest one with an acceptable error. Feel free to experiment with the number of epochs and see what happens at different numbers. . So what did I accomplish? . I&#39;m able to give the neural network the number 5, and have it output a number very close to our goal number 42 without putting the number 5 or 42 in the body of the function. . I also learned the basic parts which make up all neural networks and we learned the process of how the network learns. . As we start to move into networks with multiple inputs, multiple outputs, and multiple layers, it&#39;s going to get a lot more complicated. However, the mental model stays the same. The network makes a prediction by multiplying the received input with its stored weights. It measures the error, takes the derivative, and adjusts the weights so that error moves towards 0. Then it goes again. . What&#39;s next? . I&#39;m going to tackle multiple inputs and multiple outputs. I&#39;ll see how matrices come into play and how we can build a simple library to do matrix math. . See you then! .",
            "url": "https://leogau.dev/2021/01/22/How-I-Implemented-The-Most-Simple-Neural-Net.html",
            "relUrl": "/2021/01/22/How-I-Implemented-The-Most-Simple-Neural-Net.html",
            "date": " ‚Ä¢ Jan 22, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "nbdev + GitHub Codespaces: A New Literate Programming Environment",
            "content": "Today, we are going to show you how to set up a literate programming environment, allowing you to use an IDE (VS Code) and an interactive computing environment (Jupyter), without leaving your browser, for free, in under 5 minutes. You‚Äôll even see how VSCode and Jupyter work together automatically! But first, what is literate programming? And how did I go from skeptic to a zealot of literate programming? . Introduction . Literate programming is a programming paradigm introduced by Donald Knuth in which a computer program is given an explanation of its logic in a natural language, such as English, interspersed with snippets of macros and traditional source code, from which compilable source code can be generated. According to Knuth, literate programming provides higher-quality programs by forcing programmers to explicitly state the thoughts behind the program. This process makes poorly thought-out design decisions more obvious. Knuth also claims that literate programming provides a first-rate documentation system, which is not an add-on, but is grown naturally in the process of exposition of one‚Äôs thoughts during a program‚Äôs creation. 1 . When I first learned about literate programming, I was quite skeptical. For the longest time, I had wrongly equated Jupyter notebooks with literate programming. Indeed, Jupyter is a brilliant interactive computing system, which was awarded the Association of Computing Machinery (ACM) Software System Award, and is loved by many developers. However, Jupyter falls short of the literate programming paradigm for the following reasons:2 . It can be difficult to compile source code from notebooks. | It can be difficult to diff and use version control with notebooks because they are not stored in plain text. | It is not clear how to automatically generate documentation from notebooks. | It is not clear how to properly run tests suites when writing code in notebooks. | . My skepticism quickly evaporated when I began using nbdev, a project that extends notebooks to complete the literate programming ideal. I spent a month, full time, using nbdev while contributing to the python library fastcore, and can report that Donald Knuth was definitely onto something. The process of writing prose and tests alongside code forced me to deeply understand why the code does what it does, and to think deeply about its design. Furthermore, the reduced cognitive load and speed of iteration of having documentation, code, and tests in one location boosted my productivity to levels I have never before experienced as a software developer. Furthermore, I found that developing this way bolstered collaboration such that code reviews not only happened faster but were more meaningful. In short, nbdev may be the most profound productivity tool I have ever used. . As a teaser, look how easy it is to instantiate this literate programming environment, which includes a notebook, a docs site and an IDE with all dependencies pre-installed! :point_down: . . Features of nbdev . As discussed in the docs, nbdev provides the following features: . Searchable, hyperlinked documentation, which can be automatically hosted on GitHub Pages for free. | Python modules, following best practices such as automatically defining __all__ with your exported functions, classes, and variables. | Pip and Conda installers. | Tests defined directly in notebooks which run in parallel. This testing system has been thoroughly tested with GitHub Actions. | Navigate and edit your code in a standard text editor or IDE, and export any changes automatically back into your notebooks. | . Since you are in a notebook, you can also add charts, text, links, images, videos, etc, that are included automatically in the documentation of your library, along with standardized documentation generated automatically from your code. This site is an example of docs generated automatically by nbdev. . GitHub Codespaces . Thanks to Conda and nbdev_template, setting up a development environment with nbdev is far easier than it used to be. However, we realized it could be even easier, thanks to a new GitHub product called Codespaces. Codespaces is a fully functional development environment in your browser, accessible directly from GitHub, that provides the following features: . A full VS Code IDE. | An environment that has files from the repository mounted into the environment, along with your GitHub credentials. | A development environment with dependencies pre-installed, backed by Docker. | The ability to serve additional applications on arbitrary ports. For nbdev, we serve a Jupyter notebook server as well as a Jekyll based documentation site. | A shared file system, which facilitates editing code in one browser tab and rendering the results in another. | ‚Ä¶ and more. | Codespaces enables developers to immediately participate in a project without wasting time on DevOps or complicated setup steps. Most importantly, CodeSpaces with nbdev allows developers to quickly get started with creating their own software with literate programming. . A demo of nbdev + Codespaces . This demo uses the project fastai/fastcore, which was built with nbdev, as an example. First, we can navigate to this repo and launch a Codespace: . . If you are launching a fresh Codespace, it may take several minutes to set up. Once the environment is ready, we can verify that all dependencies we want are installed (in this case fastcore and nbdev): . . Additionally, we can serve an arbitrary number of applications on user-specified ports, which we can open through VSCode as shown below: . . In this case, these applications are a notebook and docs site. Changes to a notebook are reflected immediately in the data docs. Furthermore, we can use the cli command nbdev_build_lib to sync our notebooks with python modules. This functionality is shown below: . . This is amazing! With a click of a button, I was able to: . Launch an IDE with all dependencies pre-installed. | Launch two additional applications: a Jupyter Notebook server on port 8080 and a docs site on port 4000. | Automatically update the docs and modules every time I make a change to a Jupyter notebook. | This is just the tip of the iceberg. There are additional utilities for writing and executing tests, diffing notebooks, special flags for hiding, showing, and collapsing cells in the generated docs, as well as git hooks for automation. This and more functionality is covered in the nbdev docs. . Give It A Try For Yourself . To try out nbdev yourself, take this tutorial, which will walk you through everything you need to know. The tutorial also shows you how to use a repository template with the configuration files necessary to enable Codespaces with nbdev. . You Can Write Blogs With Notebooks, Too! . This blog post was written in fastpages which is also built on nbdev! We recommend fastpages if you want an easy way to blog with Jupyter notebooks. . Additional Resources . The GitHub Codepaces site. | The official docs for Codespaces. | The nbdev docs. | The nbdev GitHub repo. | fastpages: The project used to write this blog. | The GitHub repo fastai/fastcore, which is what we used in this blog post as an example. | . Wikipedia article: Literate Programming¬†&#8617; . | This is not a criticism of Jupyter. Jupyter doesn‚Äôt claim to be a full literate programming system. However, people can sometimes (unfairly) judge Jupyter according to this criteria.¬†&#8617; . |",
            "url": "https://leogau.dev/codespaces",
            "relUrl": "/codespaces",
            "date": " ‚Ä¢ Dec 10, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Introducing fastlinkcheck",
            "content": ". Motivation . Recently, fastai has been hard at work improving and overhauling nbdev, a literate programming environment for python. A key feature of nbdev is automated generation of documentation from Jupyter notebooks. This documentation system adds many niceties, such as the following types of hyperlinks automatically: . Links to source code on GitHub. | Links to both internal and external documentation by introspecting variable names in backticks. | . Because documentation is so easy to create and maintain in nbdev, we find ourselves and others creating much more of it! In addition to automatic hyperlinks, we often include our own links to relevant websites, blogs and videos when documenting code. For example, one of the largest nbdev generated sites, docs.fast.ai, has more than 300 external and internal links at the time of this writing. . The Solution . Due to the continued popularity of fastai and the growth of new nbdev projects, grooming these links manually became quite tedious. We investigated solutions that could verify links for us automatically, but were not satisfied with any existing solutions. These are the features we desired: . A platform independent solution that is not tied to a specific static site generator like Jekyll or Hugo. | Intelligent introspection of external links that are actually internal links. For example, if we are building the site docs.fast.ai, a link to https://docs.fast.ai/tutorial should not result in a web request, but rather introspection of the local file system for the presence of tutorial.html in the right location. | Verification of any links to assets like CSS, data, javascript or other files. | Logs that are well organized that allow us to see each broken link or reference to a non-existent path, and the pages these are found in. | Parallelism to verify links as fast as possible. | Lightweight, easy to install with minimal dependencies. | . We tried tools such as linkchecker and pylinkvalidator, but these required your site to be first be hosted. Since we wanted to check links on a static site, hosting is overhead we wanted to avoid. . This is what led us to create fastlinkcheck, which we discuss below. . Note: For Ruby users, htmlproofer apperas to provide overlapping functionality. We have not tried this library. . A tour of fastlinkcheck . For this tour we will be referring to the files in the fastlinkcheck repo. You should clone this repo in the current directory in order to follow along: . git clone https://github.com/fastai/fastlinkcheck.git cd fastlinkcheck . Cloning into &#39;fastlinkcheck&#39;... remote: Enumerating objects: 135, done. remote: Counting objects: 100% (135/135), done. remote: Compressing objects: 100% (98/98), done. remote: Total 608 (delta 69), reused 76 (delta 34), pack-reused 473 Receiving objects: 100% (608/608), 1.12 MiB | 10.47 MiB/s, done. Resolving deltas: 100% (302/302), done. . Installation . You can install fastlinkcheck with pip: . pip install fastlinkcheck . Usage . After installing fastlinkcheck, the cli command link_check is available from the command line. We can see various options with the --help flag. . link_check --help . usage: link_check [-h] [--host HOST] [--config_file CONFIG_FILE] [--pdb] [--xtra XTRA] path Check for broken links recursively in `path`. positional arguments: path Root directory searched recursively for HTML files optional arguments: -h, --help show this help message and exit --host HOST Host and path (without protocol) of web server --config_file CONFIG_FILE Location of file with urls to ignore --pdb Run in pdb debugger (default: False) --xtra XTRA Parse for additional args (default: &#39;&#39;) . From the root of fastlinkcheck repo, We can search the directory _example/broken_links recursively for broken links like this: . link_check _example/broken_links . ERROR: The Following Broken Links or Paths were found: - &#39;http://fastlinkcheck.com/test.html&#39; was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` - &#39;http://somecdn.com/doesntexist.html&#39; was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` - Path(&#39;/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.js&#39;) was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` . . Specifying the --host parameter allows you detect links that are internal by identifying links with that host name. External links are verified by making a request to the appropriate website. On the other hand, internal links are verified by inspecting the presence and content of local files. . We must be careful when using the --host argument to only pass the host (and path, if necessary) without the protocol. For example, this is how we specify the hostname if your site&#39;s url is http://fastlinkcheck.com/test.html: . link_check _example/broken_links --host fastlinkcheck.com . ERROR: The Following Broken Links or Paths were found: - &#39;http://somecdn.com/doesntexist.html&#39; was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` - Path(&#39;/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.js&#39;) was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` . . We now have one less broken link as there is indeed a file named test.html in the root of the path we are searching. However, if we add a path to the end of --host , such as fastlinkcheck.com/mysite the link would again be listed as broken because _example/broken_links/mysite/test.html does not exist: . link_check _example/broken_links --host fastlinkcheck.com/mysite . ERROR: The Following Broken Links or Paths were found: - &#39;http://fastlinkcheck.com/test.html&#39; was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` - &#39;http://somecdn.com/doesntexist.html&#39; was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` - Path(&#39;/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.js&#39;) was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` . . You can ignore links by creating a text file that contains a list of urls and paths to ignore. For example, the file _example/broken_links/linkcheck.rc contains: . cat _example/broken_links/linkcheck.rc . test.js https://www.google.com . We can use this file to ignore urls and paths with the --config_file argument. This will filter out references to the broken link /test.js from our earlier results: . link_check _example/broken_links --host fastlinkcheck.com --config_file _example/broken_links/linkcheck.rc . ERROR: The Following Broken Links or Paths were found: - &#39;http://somecdn.com/doesntexist.html&#39; was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` . . Finally, if there are no broken links, link_check will not return anything. The directory _example/no_broken_links/ does not contain any HTML files with broken links: . link_check _example/no_broken_links . ‚ñà |--| 0.00% [0/2 00:00&lt;00:00] |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà-| 50.00% [1/2 00:00&lt;00:00] |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100.00% [2/2 00:00&lt;00:00] No broken links found! . Python . You can also use these utilities from python instead of the terminal. Please see these docs for more information. . Using link_check in GitHub Actions . The link_check CLI utility that is installed with fastlinkcheck can be very useful in continuous integration systems like GitHub Actions. Here is an example GitHub Actions workflow that uses link_check: . name: Check Links on: [workflow_dispatch, push] jobs: check-links: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - uses: actions/setup-python@v2 - name: check for broken links run: | pip install fastlinkcheck link_check _example . We can a few more lines of code to open an issue instead when a broken link is found, using the gh cli: . ... - name: check for broken links run: | pip install fastlinkcheck link_check _example 2&gt; err || true export GITHUB_TOKEN=&quot;YOUR_TOKEN&quot; [[ -s err ]] &amp;&amp; gh issue create -t &quot;Broken links found&quot; -b &quot;$(&lt; err)&quot; -R &quot;yourusername/yourrepo&quot; . We can extend this even further to only open an issue when another issue with a specific label isn&#39;t already open: . ... - name: check for broken links run: | pip install fastlinkcheck link_check &quot;docs/_site&quot; --host &quot;docs.fast.ai&quot; 2&gt; err || true export GITHUB_TOKEN=&quot;YOUR_TOKEN&quot; if [[ -z $(gh issue list -l &quot;broken-link&quot;)) &amp;&amp; (-s err) ]]; then gh issue create -t &quot;Broken links found&quot; -b &quot;$(&lt; err)&quot; -l &quot;broken-link&quot; -R &quot;yourusername/yourrepo&quot; fi . See the GitHub Actions docs for more information. . Resources . The following resources are relevant for those interested in learning more about fastlinkcheck: . The fastlinkcheck GitHub repo | The fastlinkcheck docs | .",
            "url": "https://leogau.dev/fastlinkcheck/",
            "relUrl": "/fastlinkcheck/",
            "date": " ‚Ä¢ Nov 17, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "fastcore: An Underrated Python Library",
            "content": ". Background . I recently embarked on a journey to sharpen my python skills: I wanted to learn advanced patterns, idioms, and techniques. I started with reading books on advanced Python, however, the information didn&#39;t seem to stick without having somewhere to apply it. I also wanted the ability to ask questions from an expert while I was learning -- which is an arrangement that is hard to find! That&#39;s when it occurred to me: What if I could find an open source project that has fairly advanced python code and write documentation and tests? I made a bet that if I did this it would force me to learn everything very deeply, and the maintainers would be appreciative of my work and be willing to answer my questions. . And that&#39;s exactly what I did over the past month! I&#39;m pleased to report that it has been the most efficient learning experience I&#39;ve ever experienced. I&#39;ve discovered that writing documentation forced me to deeply understand not just what the code does but also why the code works the way it does, and to explore edge cases while writing tests. Most importantly, I was able to ask questions when I was stuck, and maintainers were willing to devote extra time knowing that their mentorship was in service of making their code more accessible! It turns out the library I choose, fastcore is some of the most fascinating Python I have ever encountered as its purpose and goals are fairly unique. . For the uninitiated, fastcore is a library on top of which many fast.ai projects are built on. Most importantly, fastcore extends the python programming language and strives to eliminate boilerplate and add useful functionality for common tasks. In this blog post, I&#39;m going to highlight some of my favorite tools that fastcore provides, rather than sharing what I learned about python. My goal is to pique your interest in this library, and hopefully motivate you to check out the documentation after you are done to learn more! . Why fastcore is interesting . Get exposed to ideas from other languages without leaving python: I‚Äôve always heard that it is beneficial to learn other languages in order to become a better programmer. From a pragmatic point of view, I‚Äôve found it difficult to learn other languages because I could never use them at work. Fastcore extends python to include patterns found in languages as diverse as Julia, Ruby and Haskell. Now that I understand these tools I am motivated to learn other languages. | You get a new set of pragmatic tools: fastcore includes utilities that will allow you to write more concise expressive code, and perhaps solve new problems. | Learn more about the Python programming language: Because fastcore extends the python programming language, many advanced concepts are exposed during the process. For the motivated, this is a great way to see how many of the internals of python work. | A whirlwind tour through fastcore . Here are some things you can do with fastcore that immediately caught my attention. . . Making **kwargs transparent . Whenever I see a function that has the argument **kwargs, I cringe a little. This is because it means the API is obfuscated and I have to read the source code to figure out what valid parameters might be. Consider the below example: . def baz(a, b=2, c=3, d=4): return a + b + c def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, **kwargs)&gt; . Without reading the source code, it might be hard for me to know that foo also accepts and additional parameters b and d. We can fix this with delegates: . def baz(a, b=2, c=3, d=4): return a + b + c @delegates(baz) # this decorator will pass down keyword arguments from baz def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, b=2, d=4)&gt; . You can customize the behavior of this decorator. For example, you can have your cake and eat it too by passing down your arguments and also keeping **kwargs: . @delegates(baz, keep=True) def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, b=2, d=4, **kwargs)&gt; . You can also exclude arguments. For example, we exclude argument d from delegation: . def basefoo(a, b=2, c=3, d=4): pass @delegates(basefoo, but=[&#39;d&#39;]) # exclude `d` def foo(c, a, **kwargs): pass inspect.signature(foo) . &lt;Signature (c, a, b=2)&gt; . You can also delegate between classes: . class BaseFoo: def __init__(self, e, c=2): pass @delegates()# since no argument was passsed here we delegate to the superclass class Foo(BaseFoo): def __init__(self, a, b=1, **kwargs): super().__init__(**kwargs) inspect.signature(Foo) . &lt;Signature (a, b=1, c=2)&gt; . For more information, read the docs on delegates. . . Avoid boilerplate when setting instance attributes . Have you ever wondered if it was possible to avoid the boilerplate involved with setting attributes in __init__? . class Test: def __init__(self, a, b ,c): self.a, self.b, self.c = a, b, c . Ouch! That was painful. Look at all the repeated variable names. Do I really have to repeat myself like this when defining a class? Not Anymore! Checkout store_attr: . class Test: def __init__(self, a, b, c): store_attr() t = Test(5,4,3) assert t.b == 4 . You can also exclude certain attributes: . class Test: def __init__(self, a, b, c): store_attr(but=[&#39;c&#39;]) t = Test(5,4,3) assert t.b == 4 assert not hasattr(t, &#39;c&#39;) . There are many more ways of customizing and using store_attr than I highlighted here. Check out the docs for more detail. . P.S. you might be thinking that Python dataclasses also allow you to avoid this boilerplate. While true in some cases, store_attr is more flexible.1 . 1. For example, store_attr does not rely on inheritance, which means you won&#39;t get stuck using multiple inheritance when using this with your own classes. Also, unlike dataclasses, store_attr does not require python 3.7 or higher. Furthermore, you can use store_attr anytime in the object lifecycle, and in any location in your class to customize the behavior of how and when variables are stored.‚Ü© . . Avoiding subclassing boilerplate . One thing I hate about python is the __super__().__init__() boilerplate associated with subclassing. For example: . class ParentClass: def __init__(self): self.some_attr = &#39;hello&#39; class ChildClass(ParentClass): def __init__(self): super().__init__() cc = ChildClass() assert cc.some_attr == &#39;hello&#39; # only accessible b/c you used super . We can avoid this boilerplate by using the metaclass PrePostInitMeta. We define a new class called NewParent that is a wrapper around the ParentClass: . class NewParent(ParentClass, metaclass=PrePostInitMeta): def __pre_init__(self, *args, **kwargs): super().__init__() class ChildClass(NewParent): def __init__(self):pass sc = ChildClass() assert sc.some_attr == &#39;hello&#39; . . Type Dispatch . Type dispatch, or Multiple dispatch, allows you to change the way a function behaves based upon the input types it receives. This is a prominent feature in some programming languages like Julia. For example, this is a conceptual example of how multiple dispatch works in Julia, returning different values depending on the input types of x and y: . collide_with(x::Asteroid, y::Asteroid) = ... # deal with asteroid hitting asteroid collide_with(x::Asteroid, y::Spaceship) = ... # deal with asteroid hitting spaceship collide_with(x::Spaceship, y::Asteroid) = ... # deal with spaceship hitting asteroid collide_with(x::Spaceship, y::Spaceship) = ... # deal with spaceship hitting spaceship . Type dispatch can be especially useful in data science, where you might allow different input types (i.e. Numpy arrays and Pandas dataframes) to a function that processes data. Type dispatch allows you to have a common API for functions that do similar tasks. . Unfortunately, Python does not support this out-of-the box. Fortunately, there is the @typedispatch decorator to the rescue. This decorator relies upon type hints in order to route inputs the correct version of the function: . @typedispatch def f(x:str, y:str): return f&#39;{x}{y}&#39; @typedispatch def f(x:np.ndarray): return x.sum() @typedispatch def f(x:int, y:int): return x+y . Below is a demonstration of type dispatch at work for the function f: . f(&#39;Hello &#39;, &#39;World!&#39;) . &#39;Hello World!&#39; . f(2,3) . 5 . f(np.array([5,5,5,5])) . 20 . There are limitations of this feature, as well as other ways of using this functionality that you can read about here. In the process of learning about typed dispatch, I also found a python library called multipledispatch made by Mathhew Rocklin (the creator of Dask). . After using this feature, I am now motivated to learn languages like Julia to discover what other paradigms I might be missing. . . A better version of functools.partial . functools.partial is a great utility that creates functions from other functions that lets you set default values. Lets take this function for example that filters a list to only contain values &gt;= val: . test_input = [1,2,3,4,5,6] def f(arr, val): &quot;Filter a list to remove any values that are less than val.&quot; return [x for x in arr if x &gt;= val] f(test_input, 3) . [3, 4, 5, 6] . You can create a new function out of this function using partial that sets the default value to 5: . filter5 = partial(f, val=5) filter5(test_input) . [5, 6] . One problem with partial is that it removes the original docstring and replaces it with a generic docstring: . filter5.__doc__ . &#39;partial(func, *args, **keywords) - new function with partial application n of the given arguments and keywords. n&#39; . fastcore.utils.partialler fixes this, and makes sure the docstring is retained such that the new API is transparent: . filter5 = partialler(f, val=5) filter5.__doc__ . &#39;Filter a list to remove any values that are less than val.&#39; . . Composition of functions . A technique that is pervasive in functional programming languages is function composition, whereby you chain a bunch of functions together to achieve some kind of result. This is especially useful when applying various data transformations. Consider a toy example where I have three functions: (1) Removes elements of a list less than 5 (from the prior section) (2) adds 2 to each number (3) sums all the numbers: . def add(arr, val): return [x + val for x in arr] def arrsum(arr): return sum(arr) # See the previous section on partialler add2 = partialler(add, val=2) transform = compose(filter5, add2, arrsum) transform([1,2,3,4,5,6]) . 15 . But why is this useful? You might me thinking, I can accomplish the same thing with: . arrsum(add2(filter5([1,2,3,4,5,6]))) . You are not wrong! However, composition gives you a convenient interface in case you want to do something like the following: . def fit(x, transforms:list): &quot;fit a model after performing transformations&quot; x = compose(*transforms)(x) y = [np.mean(x)] * len(x) # its a dumb model. Don&#39;t judge me return y # filters out elements &lt; 5, adds 2, then predicts the mean fit(x=[1,2,3,4,5,6], transforms=[filter5, add2]) . [7.5, 7.5] . For more information about compose, read the docs. . . A more useful __repr__ . In python, __repr__ helps you get information about an object for logging and debugging. Below is what you get by default when you define a new class. (Note: we are using store_attr, which was discussed earlier). . class Test: def __init__(self, a, b=2, c=3): store_attr() # `store_attr` was discussed previously Test(1) . &lt;__main__.Test at 0x7ffcd766cee0&gt; . We can use basic_repr to quickly give us a more sensible default: . class Test: def __init__(self, a, b=2, c=3): store_attr() __repr__ = basic_repr(&#39;a,b,c&#39;) Test(2) . Test(a=2, b=2, c=3) . . Monkey Patching With A Decorator . It can be convenient to monkey patch with a decorator, which is especially helpful when you want to patch an external library you are importing. We can use the decorator @patch from fastcore.foundation along with type hints like so: . class MyClass(int): pass @patch def func(self:MyClass, a): return self+a mc = MyClass(3) . Now, MyClass has an additional method named func: . mc.func(10) . 13 . Still not convinced? I&#39;ll show you another example of this kind of patching in the next section. . . A better pathlib.Path . When you see these extensions to pathlib.path you won&#39;t ever use vanilla pathlib again! A number of additional methods have been added to pathlib, such as: . Path.readlines: same as with open(&#39;somefile&#39;, &#39;r&#39;) as f: f.readlines() | Path.read: same as with open(&#39;somefile&#39;, &#39;r&#39;) as f: f.read() | Path.save: saves file as pickle | Path.load: loads pickle file | Path.ls: shows the contents of the path as a list. | etc. | . Read more about this here. Here is a demonstration of ls: . from fastcore.utils import * from pathlib import Path p = Path(&#39;.&#39;) p.ls() # you don&#39;t get this with vanilla Pathlib.Path!! . (#7) [Path(&#39;2020-09-01-fastcore.ipynb&#39;),Path(&#39;README.md&#39;),Path(&#39;fastcore_imgs&#39;),Path(&#39;2020-02-20-test.ipynb&#39;),Path(&#39;.ipynb_checkpoints&#39;),Path(&#39;2020-02-21-introducing-fastpages.ipynb&#39;),Path(&#39;my_icons&#39;)] . Wait! What&#39;s going on here? We just imported pathlib.Path - why are we getting this new functionality? Thats because we imported the fastcore.utils module, which patches this module via the @patch decorator discussed earlier. Just to drive the point home on why the @patch decorator is useful, I&#39;ll go ahead and add another method to Path right now: . @patch def fun(self:Path): return &quot;This is fun!&quot; p.fun() . &#39;This is fun!&#39; . That is magical, right? I know! That&#39;s why I&#39;m writing about it! . . An Even More Concise Way To Create Lambdas . Self, with an uppercase S, is an even more concise way to create lambdas that are calling methods on an object. For example, let&#39;s create a lambda for taking the sum of a Numpy array: . arr=np.array([5,4,3,2,1]) f = lambda a: a.sum() assert f(arr) == 15 . You can use Self in the same way: . f = Self.sum() assert f(arr) == 15 . Let&#39;s create a lambda that does a groupby and max of a Pandas dataframe: . import pandas as pd df=pd.DataFrame({&#39;Some Column&#39;: [&#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;, ], &#39;Another Column&#39;: [5, 7, 50, 70]}) f = Self.groupby(&#39;Some Column&#39;).mean() f(df) . Another Column . Some Column . a 6 | . b 60 | . Read more about Self in the docs). . . Notebook Functions . These are simple but handy, and allow you to know whether or not code is executing in a Jupyter Notebook, Colab, or an Ipython Shell: . from fastcore.imports import in_notebook, in_colab, in_ipython in_notebook(), in_colab(), in_ipython() . (True, False, True) . This is useful if you are displaying certain types of visualizations, progress bars or animations in your code that you may want to modify or toggle depending on the environment. . . A Drop-In Replacement For List . You might be pretty happy with Python&#39;s list. This is one of those situations that you don&#39;t know you needed a better list until someone showed one to you. Enter L, a list like object with many extra goodies. . The best way I can describe L is to pretend that list and numpy had a pretty baby: . define a list (check out the nice __repr__ that shows the length of the list!) . L(1,2,3) . (#3) [1,2,3] . Shuffle a list: . p = L.range(20).shuffle() p . (#20) [8,7,5,12,14,16,2,15,19,6...] . Index into a list: . p[2,4,6] . (#3) [5,14,2] . L has sensible defaults, for example appending an element to a list: . 1 + L(2,3,4) . (#4) [1,2,3,4] . There is much more L has to offer. Read the docs to learn more. . But Wait ... There&#39;s More! . There are more things I would like to show you about fastcore, but there is no way they would reasonably fit into a blog post. Here is a list of some of my favorite things that I didn&#39;t demo in this blog post: . Utilities . The Basics section contain many shortcuts to perform common tasks or provide an additional interface to what standard python provides. . mk_class: quickly add a bunch of attributes to a class | wrap_class: add new methods to a class with a simple decorator | groupby: similar to Scala&#39;s groupby | merge: merge dicts | fasttuple: a tuple on steroids | Infinite Lists: useful for padding and testing | chunked: for batching and organizing stuff | . Multiprocessing . The Multiprocessing section extends python&#39;s multiprocessing library by offering features like: . progress bars | ability to pause to mitigate race conditions with external services | processing things in batches on each worker, ex: if you have a vectorized operation to perform in chunks | . Functional Programming . The functional programming section is my favorite part of this library. . maps: a map that also composes functions | mapped: A more robust map | using_attr: compose a function that operates on an attribute | . Transforms . Transforms is a collection of utilities for creating data transformations and associated pipelines. These transformation utilities build upon many of the building blocks discussed in this blog post. . Further Reading . It should be noted that you should read the main page of the docs first, followed by the section on tests to fully understand the documentation. . The fastcore documentation site. | The fastcore GitHub repo. | Blog post on delegation. | . Shameless plug: fastpages . This blog post was written entirely in a Jupyter Notebook, which GitHub automatically converted into to a blog post! Sound interesting? Check out fastpages. .",
            "url": "https://leogau.dev/fastcore/",
            "relUrl": "/fastcore/",
            "date": " ‚Ä¢ Sep 1, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "GitHub Actions: Providing Data Scientists With New Superpowers",
            "content": "What Superpowers? . Hi, I‚Äôm Hamel Husain. I‚Äôm a machine learning engineer at GitHub. Recently, GitHub released a new product called GitHub Actions, which has mostly flown under the radar in the machine learning and data science community as just another continuous integration tool. . Recently, I‚Äôve been able to use GitHub Actions to build some very unique tools for Data Scientists, which I want to share with you today. Most importantly, I hope to get you excited about GitHub Actions, and the promise it has for giving you new superpowers as a Data Scientist. Here are two projects I recently built with Actions that show off its potential: . fastpages . fastpages is an automated, open-source blogging platform with enhanced support for Jupyter notebooks. You save your notebooks, markdown, or Word docs into a directory on GitHub, and they automatically become blog posts. Read the announcement below: . We&#39;re launching `fastpages`, a platform which allows you to host a blog for free, with no ads. You can blog with @ProjectJupyter notebooks, @office Word, directly from @github&#39;s markdown editor, etc.Nothing to install, &amp; setup is automated!https://t.co/dNSA0oQUrN . &mdash; Jeremy Howard (@jeremyphoward) February 24, 2020 Machine Learning Ops . Wouldn‚Äôt it be cool if you could invoke a chatbot natively on GitHub to test your machine learning models on the infrastructure of your choice (GPUs), log all the results, and give you a rich report back in a pull request so that everyone could see the results? You can with GitHub Actions! . Consider the below annotated screenshot of this Pull Request: . . A more in-depth explanation about the above project can be viewed in this video: . Using GitHub Actions for machine learning workflows is starting to catch on. Julien Chaumond, CTO of Hugging Face, says: . GitHub Actions are great because they let us do CI on GPUs (as most of our users use the library on GPUs not on CPUs), on our own infra! 1 . Additionally, you can host a GitHub Action for other people so others can use parts of your workflow without having to re-create your steps. I provide examples of this below. . A Gentle Introduction To GitHub Actions . What Are GitHub Actions? . GitHub Actions allow you to run arbitrary code in response to events. Events are activities that happen on GitHub such as: . Opening a pull request | Making an issue comment | Labeling an issue | Creating a new branch | ‚Ä¶ and many more | . When an event is created, the GitHub Actions context is hydrated with a payload containing metadata for that event. Below is an example of a payload that is received when an issue is created: . { &quot;action&quot;: &quot;created&quot;, &quot;issue&quot;: { &quot;id&quot;: 444500041, &quot;number&quot;: 1, &quot;title&quot;: &quot;Spelling error in the README file&quot;, &quot;user&quot;: { &quot;login&quot;: &quot;Codertocat&quot;, &quot;type&quot;: &quot;User&quot;, }, &quot;labels&quot;: [ { &quot;id&quot;: 1362934389, &quot;node_id&quot;: &quot;MDU6TGFiZWwxMzYyOTM0Mzg5&quot;, &quot;name&quot;: &quot;bug&quot;, } ], &quot;body&quot;: &quot;It looks like you accidently spelled &#39;commit&#39; with two &#39;t&#39;s.&quot; } . This functionality allows you to respond to various events on GitHub in an automated way. In addition to this payload, GitHub Actions also provide a plethora of variables and environment variables that afford easy to access metadata such as the username and the owner of the repo. Additionally, other people can package useful functionality into an Action that other people can inherit. For example, consider the below Action that helps you publish python packages to PyPi: . The Usage section describes how this Action can be used: . - name: Publish a Python distribution to PyPI uses: pypa/gh-action-pypi-publish@master with: user: __token__ password: ${{ secrets.pypi_password }} . This Action expects two inputs: user and a password. You will notice that the password is referencing a variable called secrets, which is a variable that contains an encrypted secret that you can upload to your GitHub repository. There are thousands of Actions (that are free) for a wide variety of tasks that can be discovered on the GitHub Marketplace. The ability to inherit ready-made Actions in your workflow allows you to accomplish complex tasks without implementing all of the logic yourself. Some useful Actions for those getting started are: . actions/checkout: Allows you to quickly clone the contents of your repository into your environment, which you often want to do. This does a number of other things such as automatically mount your repository‚Äôs files into downstream Docker containers. | mxschmitt/action-tmate: Proivdes a way to debug Actions interactively. This uses port forwarding to give you a terminal in the browser that is connected to your Actions runner. Be careful not to expose sensitive information if you use this. | actions/github-script: Gives you a pre-authenticated ocotokit.js client that allows you to interact with the GitHub API to accomplish almost any task on GitHub automatically. Only these endpoints are supported (for example, the secrets endpoint is not in that list). | . In addition to the aforementioned Actions, it is helpful to go peruse the official GitHub Actions docs before diving in. . Example: A fastpages Action Workflow . The best to way familiarize yourself with Actions is by studying examples. Let‚Äôs take a look at the Action workflow that automates the build of fastpages (the platform used to write this blog post). . Part 1: Define Workflow Triggers . First, we define triggers in ci.yaml. Like all Actions workflows, this is a YAML file located in the .github/workflows directory of the GitHub repo. . The top of this YAML file looks like this: . name: CI on: push: branches: - master pull_request: . This means that this workflow is triggered on either a push or pull request event. Furthermore, push events are filtered such that only pushes to the master branch will trigger the workflow, whereas all pull requests will trigger this workflow. It is important to note that pull requests opened from forks will have read-only access to the base repository and cannot access any secrets for security reasons. The reason for defining the workflow in this way is we wanted to trigger the same workflow to test pull requests as well as build and deploy the website when a PR is merged into master. This will be clarified as we step through the rest of the YAML file. . Part 2: Define Jobs . Next, we define jobs (there is only one in this workflow). Per the docs: . A workflow run is made up of one or more jobs. Jobs run in parallel by default. . jobs: build-site: if: ( github.event.commits[0].message != &#39;Initial commit&#39; ) || github.run_number &gt; 1 runs-on: ubuntu-latest steps: . The keyword build-site is the name of your job and you can name it whatever you want. In this case, we have a conditional if statement that dictates if this job should be run or not. We are trying to ensure that this workflow does not run when the first commit to a repo is made with the message ‚ÄòInitial commit‚Äô. The first variable in the if statement, github.event, contains a json payload of the event that triggered this workflow. When developing workflows, it is helpful to print this variable in order to inspect its structure, which you can accomplish with the following YAML: . - name: see payload run: | echo &quot;PAYLOAD: n${PAYLOAD} n&quot; env: PAYLOAD: ${{ toJSON(github.event) }} . Note: the above step is only for debugging and is not currently in the workflow. . toJson is a handy function that returns a pretty-printed JSON representation of the variable. The output is printed directly in the logs contained in the Actions tab of your repo. In this example, printing the payload for a push event will look like this (truncated for brevity): . { &quot;ref&quot;: &quot;refs/tags/simple-tag&quot;, &quot;before&quot;: &quot;6113728f27ae8c7b1a77c8d03f9ed6e0adf246&quot;, &quot;created&quot;: false, &quot;deleted&quot;: true, &quot;forced&quot;: false, &quot;base_ref&quot;: null, &quot;commits&quot;: [ { &quot;message&quot;: &quot;updated README.md&quot;, &quot;author&quot;: &quot;hamelsmu&quot; }, ], &quot;head_commit&quot;: null, } . Therefore, the variable github.event.commits[0].message will retrieve the first commit message in the array of commits. Since we are looking for situations where there is only one commit, this logic suffices. The second variable in the if statement, github.run_number is a special variable in Actions which: . [is a] unique number for each run of a particular workflow in a repository. This number begins at 1 for the workflow‚Äôs first run, and increments with each new run. This number does not change if you re-run the workflow run. . Therefore, the if statement introduced above: . if: ( github.event.commits[0].message != &#39;Initial commit&#39; ) || github.run_number &gt; 1 . Allows the workflow to run when the commit message is ‚ÄúInitial commit‚Äù as long as it is not the first commit. ( || is a logical or operator). . Finally, the line runs-on: ubuntu-latest specifies the host operating system that your workflows will run in. . Part 3: Define Steps . Per the docs: . A job contains a sequence of tasks called steps. Steps can run commands, run setup tasks, or run an Action in your repository, a public repository, or an Action published in a Docker registry. Not all steps run Actions, but all Actions run as a step. Each step runs in its own process in the runner environment and has access to the workspace and filesystem. Because steps run in their own process, changes to environment variables are not preserved between steps. GitHub provides built-in steps to set up and complete a job. . Below are the first two steps in our workflow: . - name: Copy Repository Contents uses: actions/checkout@master with: persist-credentials: false - name: convert notebooks and word docs to posts uses: ./_action_files . The first step creates a copy of your repository in the Actions file system, with the help of the utility action/checkout. This utility only fetches the last commit by default and saves files into a directory (whose path is stored in the environment variable GITHUB_WORKSPACE that is accessible by subsequent steps in your job. The second step runs the fastai/fastpages Action, which converts notebooks and word documents to blog posts automatically. In this case, the syntax: . uses: ./_action_files . is a special case where the pre-made GitHub Action we want to run happens to be defined in the same repo that runs this workflow. This syntax allows us to test changes to this pre-made Action when evaluating PRs by referencing the directory in the current repository that defines that pre-made Action. Note: Building pre-made Actions is beyond the scope of this tutorial. . The next three steps in our workflow are defined below: . - name: setup directories for Jekyll build run: | rm -rf _site sudo chmod -R 777 . - name: Jekyll build uses: docker://fastai/fastpages-jekyll with: args: bash -c &quot;gem install bundler &amp;&amp; jekyll build -V&quot; env: JEKYLL_ENV: &#39;production&#39; - name: copy CNAME file into _site if CNAME exists run: | sudo chmod -R 777 _site/ cp CNAME _site/ 2&gt;/dev/null || : . The step named setup directories for Jekyll build executes shell commands that remove the _site folder in order to get rid of stale files related to the page we want to build, as well as grant permissions to all the files in our repo to subsequent steps. . The step named Jekyll build executes a docker container hosted by the Jekyll community on Dockerhub called jekyll/jekyll. For those not familiar with Docker, see this tutorial. The name of this container is called fastai/fastpages-jekyll because I‚Äôm adding some additional dependencies to jekyll/jekyll and hosting those on my DockerHub account for faster build times2. The args parameter allows you to execute arbitrary commands with the Docker container by overriding the CMD instruction in the Dockerfile. We use this Docker container hosted on Dockerhub so we don‚Äôt have to deal with installing and configuring all of the complicated dependencies for Jekyll. The files from our repo are already available in the Actions runtime due to the first step in this workflow, and are mounted into this Docker container automatically for us. In this case, we are running the command jekyll build, which builds our website and places relevant assets them into the _site folder. For more information about Jekyll, read the official docs. Finally, the env parameter allows me to pass an environment variable into the Docker container. . The final command above copies a CNAME file into the _site folder, which we need for the custom domain https://fastpages.fast.ai. Setting up custom domains are outside the scope of this article. . The final step in our workflow is defined below: . - name: Deploy if: github.event_name == &#39;push&#39; uses: peaceiris/actions-gh-pages@v3 with: deploy_key: ${{ secrets.SSH_DEPLOY_KEY }} publish_dir: ./_site . The statement . if: github.event_name == &#39;push&#39; . uses the variable github.event_name to ensure this step only runs when a push event ( in this case only pushes to the master branch trigger this workflow) occur. . This step deploys the fastpages website by copying the contents of the _site folder to the root of the gh-pages branch, which GitHub Pages uses for hosting. This step uses the peaceiris/actions-gh-pages Action, pinned at version 3. Their README describes various options and inputs for this Action. . Conclusion . We hope that this has shed some light on how we use GitHub Actions to automate fastpages. While we only covered one workflow above, we hope this provides enough intuition to understand the other workflows in fastpages. We have only scratched the surface of GitHub Actions in this blog post, but we provide other materials below for those who want to dive in deeper. We have not covered how to host an Action for other people, but you can start with these docs to learn more. . Still confused about how GitHub Actions could be used for Data Science? Here are some ideas of things you can build: . Jupyter Widgets that trigger GitHub Actions to perform various tasks on GitHub via the repository dispatch event | Integration with Pachyderm for data versioning. | Integration with your favorite cloud machine learning services, such Sagemaker, Azure ML or GCP‚Äôs AI Platform. | . Related Materials . GitHub Actions official documentation | Hello world Docker Action: A template to demonstrate how to build a Docker Action for other people to use. | Awesome Actions: A curated list of interesting GitHub Actions by topic. | A tutorial on Docker for Data Scientists. | . Getting In Touch . Please feel free to get in touch with us on Twitter: . Hamel Husain @HamelHusain | Jeremy Howard @jeremyphoward | . . Footnotes . You can see some of Hugging Face‚Äôs Actions workflows for machine learning on GitHub¬†&#8617; . | These additional dependencies are defined here, which uses the ‚Äújekyll build‚Äù command to add ruby dedpendencies from the Gemfile located at the root of the repo. Additionally, this docker image is built by another Action workflow defined here.¬†&#8617; . |",
            "url": "https://leogau.dev/2020/03/06/fastpages-actions.html",
            "relUrl": "/2020/03/06/fastpages-actions.html",
            "date": " ‚Ä¢ Mar 6, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Introducing fastpages",
            "content": ". We are very pleased to announce the immediate availability of fastpages. fastpages is a platform which allows you to create and host a blog for free, with no ads and many useful features, such as: . Create posts containing code, outputs of code (which can be interactive), formatted text, etc directly from Jupyter Notebooks; for instance see this great example post from Scott Hawley. Notebook posts support features such as: Interactive visualizations made with Altair remain interactive. | Hide or show cell input and output. | Collapsable code cells that are either open or closed by default. | Define the Title, Summary and other metadata via a special markdown cells | Ability to add links to Colab and GitHub automatically. | . | Create posts, including formatting and images, directly from Microsoft Word documents. | Create and edit Markdown posts entirely online using GitHub&#39;s built-in markdown editor. | Embed Twitter cards and YouTube videos. | Categorization of blog posts by user-supplied tags for discoverability. | ... and much more | . fastpages relies on Github pages for hosting, and Github Actions to automate the creation of your blog. The setup takes around three minutes, and does not require any technical knowledge or expertise. Due to built-in automation of fastpages, you don&#39;t have to fuss with conversion scripts. All you have to do is save your Jupyter notebook, Word document or markdown file into a specified directory and the rest happens automatically. Infact, this blog post is written in a Jupyter notebook, which you can see with the &quot;View on GitHub&quot; link above. . fast.ai have previously released a similar project called fast_template, which is even easier to set up, but does not support automatic creation of posts from Microsoft Word or Jupyter notebooks, including many of the features outlined above. . Because fastpages is more flexible and extensible, we recommend using it where possible. fast_template may be a better option for getting folks blogging who have no technical expertise at all, and will only be creating posts using Github&#39;s integrated online editor. . Setting Up Fastpages . The setup process of fastpages is automated with GitHub Actions, too! Upon creating a repo from the fastpages template, a pull request will automatically be opened (after ~ 30 seconds) configuring your blog so it can start working. The automated pull request will greet you with instructions like this: . . All you have to do is follow these instructions (in the PR you receive) and your new blogging site will be up and running! . Jupyter Notebooks &amp; Fastpages . In this post, we will cover special features that fastpages provides for Jupyter notebooks. You can also write your blog posts with Word documents or markdown in fastpages, which contain many, but not all the same features. . Options via FrontMatter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . All of the above settings are enabled in this post, so you can see what they look like! . the summary field (preceeded by &gt;) will be displayed under your title, and will also be used by social media to display as the description of your page. | toc: setting this to true will automatically generate a table of contents | badges: setting this to true will display Google Colab and GitHub links on your blog post. | comments: setting this to true will enable comments. See these instructions for more details. | author this will display the authors names. | categories will allow your post to be categorized on a &quot;Tags&quot; page, where readers can browse your post by categories. | . Markdown front matter is formatted similarly to notebooks. The differences between the two can be viewed on the fastpages README. . Code Folding . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . If you want to completely hide cells (not just collapse them), read these instructions. . Interactive Charts With Altair . Interactive visualizations made with Altair remain interactive! . We leave this below cell unhidden so you can enjoy a preview of syntax highlighting in fastpages, which uses the Dracula theme. . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;IMDB_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget IMDB_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | 6.1 | . 1 First Love, Last Rites | 10876.0 | 300000.0 | 6.9 | . 2 I Married a Strange Person | 203134.0 | 250000.0 | 6.8 | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | NaN | . 4 Slam | 1087521.0 | 1000000.0 | 3.4 | . Other Features . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Images w/Captions . You can include markdown images with captions like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Of course, the caption is optional. . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . More Examples . This tutorial contains more examples of what you can do with notebooks. . How fastpages Converts Notebooks to Blog Posts . fastpages uses nbdev to power the conversion process of Jupyter Notebooks to blog posts. When you save a notebook into the /_notebooks folder of your repository, GitHub Actions applies nbdev against those notebooks automatically. The same process occurs when you save Word documents or markdown files into the _word or _posts directory, respectively. . We will discuss how GitHub Actions work in a follow up blog post. . Resources &amp; Next Steps . We highly encourage you to start blogging with fastpages! Some resources that may be helpful: . fastpages repo - this is where you can go to create your own fastpages blog! | Fastai forums - nbdev &amp; blogging category. You can ask questions about fastpages here, as well as suggest new features. | nbdev: this project powers the conversion of Jupyter notebooks to blog posts. | . If you end up writing a blog post using fastpages, please let us know on Twitter: @jeremyphoward, @HamelHusain. .",
            "url": "https://leogau.dev/2020/02/21/introducing-fastpages.html",
            "relUrl": "/2020/02/21/introducing-fastpages.html",
            "date": " ‚Ä¢ Feb 21, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.‚Ü© . 2. This is the other footnote. You can even have a link!‚Ü© .",
            "url": "https://leogau.dev/2020/02/20/test.html",
            "relUrl": "/2020/02/20/test.html",
            "date": " ‚Ä¢ Feb 20, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a ‚Äúlevel 1 heading‚Äù in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here‚Äôs a footnote 1. Here‚Äôs a horizontal rule: . . Lists . Here‚Äôs a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes ‚Ä¶and‚Ä¶ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.¬†&#8617; . |",
            "url": "https://leogau.dev/2020/01/14/test-markdown-post.html",
            "relUrl": "/2020/01/14/test-markdown-post.html",
            "date": " ‚Ä¢ Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.¬†&#8617; . |",
          "url": "https://leogau.dev/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://leogau.dev/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}