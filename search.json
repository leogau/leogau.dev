[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/2021-03-16-mnist-deep-neural-network.html",
    "href": "posts/2021-03-16-mnist-deep-neural-network.html",
    "title": "Identify MNIST Digits With 97% Accuracy By Using A 2 Layer Neural Network",
    "section": "",
    "text": "What I’m Building\nIn this post, I’ll show you how I implemented a 2 layer neural network which is able to achieve over 97% accuracy on the MNIST test set.\nThis network builds on the work in my previous posts. If you’d like a refresher, they are: - How I Implemented The Most Simple Neural Network Using Python - How I Identify Handwritten Digits Using Only Python - How I Go From 70 Lines Of Code To Only 26 Using The NumPy Library\nNow, I’ll show you the code first and then explain some concepts you need to understand what’s going on.\n\n\nThe Code\n\nimport numpy as np\nnp.random.seed(1)\n\ndef relu(x):\n    return (x &gt; 0) * x\n\ndef relu2deriv(output):\n    return output &gt; 0\n\ndef flatten_image(image):\n    return np.array(image).reshape(1, 28*28)\n\nclass NeuralNet:\n    def __init__(self):\n        self.alpha = 0.00001\n        self.hidden_size = 1000\n        self.weights_0_1 = np.random.random((28 * 28, self.hidden_size)) * 0.0001\n        self.weights_1_2 = np.random.random((self.hidden_size, 10)) * 0.0001\n\n    def predict(self, input):\n        layer_0 = input\n        layer_1 = relu(np.dot(layer_0, self.weights_0_1))\n        layer_2 = np.dot(layer_1, self.weights_1_2)\n        return layer_2\n\n    def train(self, input, labels, epochs):\n        for i in range(epochs):\n            layer_2_error = 0\n            for j in range(len(input)):\n                layer_0 = input[j]\n                layer_1 = relu(np.dot(layer_0, self.weights_0_1))\n                layer_2 = np.dot(layer_1, self.weights_1_2)    \n                \n                label = labels[j]\n                goal = np.zeros(10)\n                goal[label] = 1\n\n                layer_2_error = np.sum((layer_2 - goal) ** 2)\n\n                layer_2_delta = (layer_2 - goal)\n                layer_1_delta = layer_2_delta.dot(self.weights_1_2.T) * relu2deriv(layer_1)\n\n                self.weights_1_2 = self.weights_1_2 - (self.alpha * layer_1.T.dot(layer_2_delta))\n                self.weights_0_1 = self.weights_0_1 - (self.alpha * layer_0.T.dot(layer_1_delta))\n            \n            print(\"Error: \" + str(layer_2_error))\n\n\nfrom keras.datasets import mnist\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nimages = x_train\nlabels = y_train\n\nprepared_images = [flatten_image(image) for image in images]\nprepared_labels = np.array(labels)\n\nnn = NeuralNet()\nnn.train(prepared_images, prepared_labels, 5)\n\ntest_set = x_test\ntest_labels = y_test\nnum_correct = 0\nfor i in range(len(test_set)):\n    prediction = nn.predict(flatten_image(test_set[i]))\n    correct = test_labels[i]\n    if np.argmax(prediction) == int(correct):\n        num_correct += 1\n\nprint(str(num_correct/len(test_set) * 100) + \"%\")\n\n\n\nWhy Do I Need More Than One Layer?\nAt their core, neural networks find correlations between the input data and target data. Sometimes there’s just no correlation to be found using the number of weights given.\nOne way to increase the accuracy is to give the network more weights to use. And the way to give it more weights is to add more layers.\nWe might not have correlation between the input and output layers but we can create an extra layer to help us out.\nNow, even with more weights, there’s another problem. For any 3 layer network, there is a 2 layer network which can do that exact same thing.\nSince there is no special processing at the extra layer, it’s not contributing any new information to the network. It correlates 1:1 with the input layer. What we need is for the middle layer to sometime correlate and sometimes not correlate with the input layer. We need it to have it’s own processing.\nThis is called conditional correlation. One way to create conditional correlation is to turn off the node when the value would be negative. If the value is negative, it would normally be negatively correlated with the input. However, if we turn it off (set the value to 0) then it doesn’t effect the output at all.\nThis means a node can be selectively correlated with inputs.\nLet me flesh this out with an example.\nLet’s say a node has 2 inputs, left and right. The left input is 1 and the right input is -1. If we use both weights, the node would be 0. They cancel each other out. However, if we set right input to 0, then the node would only be correlated with the left input value. The node is now adding additional information to the network by saying, “Make me perfectly correlated with the left input, but only if the right input is 0.”\nThis wasn’t possible earlier. This is the power of adding layers to the network.\nThe technical term for a situation where 2 variables are not predictable from a straight line is “nonlinearity”. The functions we use to create nonlinearities are called activation functions. The one I use in my network - turn off the node when it’s value would be negative - is called Rectified Linear Unit (ReLU).\nSo that’s one piece of the puzzle. I add another layer to give the neural network more weights to play with and create conditional correlation with activation functions.\nAnother problem you might be thinking about now is, how do we adjust the weights of the new layers? In a single layer network, we get the derivative of the delta and the input, but now we have weights that didn’t directly contribute to the loss.\n\n\nBackpropagation\nThe process of updating the weights in intermediate layers is called backpropagation.\nHow do you use the delta from the final layer (layer_2) to figure out how to change the weights in an intermediate layer (layer_1)? It turns out, through some calculus, I can multiply the layer_2 delta with the layer_1 inputs.\nIf I had more layers, I could keep multiplying the delta with the node input to get the weight_deltas.\nWith knowledge of activation functions and backpropagation, I can now break down what I did in code.\n\n\nImport Libraries And Get Data\nFor an explaination of this code, look at some of my previous posts.\n\nimport numpy as np\nnp.random.seed(1)\n\n\nfrom keras.datasets import mnist\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nimages = x_train\nlabels = y_train\n\n\n\nActivation Functions\nLike I mentioned above, the activation function I used is called ReLU. ReLU will set any values that would be negative to 0. Any positive values remain as is.\n\ndef relu(x):\n    return (x &gt; 0) * x\n\nDuring backpropagation, I don’t want to adjust the weights if ReLU set it to 0. Therefore, I need a function to tell me if ReLu did that or not.\nThe relu2deriv function will be used to cancel the weight adjustment if it was altered during the prediction time. If ReLU set the value to 0, the weight should not be adjusted at all.\n\ndef relu2deriv(output):\n    return output &gt; 0\n\nFinally, I have my trusty flatten_image function to prepare the data.\n\ndef flatten_image(image):\n    return np.array(image).reshape(1, 28*28)\n\nThe NeuralNet class has three new features to look at.\nFirst, it has 2 sets of weights and a hidden_size which determines the size of the hidden layer.\nself.hidden_size = 1000\nself.weights_0_1 = np.random.random((28 * 28, self.hidden_size)) * 0.0001\nself.weights_1_2 = np.random.random((self.hidden_size, 10)) * 0.0001\nA hidden_size=1000 is going to give the network a lot more weights to figure out the correlation between the images and labels.\nSecond, the prediction function now does 2 weighted sums\nlayer_0 = input\nlayer_1 = relu(np.dot(layer_0, self.weights_0_1))\nlayer_2 = np.dot(layer_1, self.weights_1_2)\nAfter setting layer_0 as the inputs, I calculate layer_1 by taking the weighted sum of layer_0 and the first set of weights, self.weights_0_1. I then use our activation function on the result to get layer_1. The final layer, layer_2, is the weighted sum of layer_1 and the second set of weights, self.weights_1_2.\nFinally, let’s look at how the weights get updated.\nlayer_2_delta = (layer_2 - goal)\nlayer_1_delta = layer_2_delta.dot(self.weights_1_2.T) * relu2deriv(layer_1)\n\nlayer_2_weight_delta = layer_1.T.dot(layer_2_delta)\nlayer_1_weight_delta = layer_0.T.dot(layer_1_delta)\n\nself.weights_1_2 = self.weights_1_2 - (self.alpha * layer_2_weight_delta)\nself.weights_0_1 = self.weights_0_1 - (self.alpha * layer_1_weight_delta)\nI get the layer_2_delta like before, getting the difference between the prediction and the goal. The layer_1_delta is derived by taking the weighted sum between the layer_2_delta and the weights connected to that layer, self.weights_1_2. I also need to use the relu2deriv function at this point to tell me if ReLU adjusted the node values or not.\nA note on the .T syntax. .T in NumPy is shorthand for transpose. It lets me reshape the matrix so everything lines up correctly to do matrix math.\nSo that’s the deltas, but how much do I adjust the weights by?\nI find the layer_2_weight_delta but calculating the weighted sum of layer_1 and layer_2_delta. So it’s the inputs into the layer and the delta at that layer. The layer_1_weight_delta is the same. I calculate the weighted sum of the inputs into that layer, layer_0, and the delta, layer_1_delta.\nThe weights are still adjusted by subtracting the weight deltas multiplied by some learning rate (self.alpha).\n\nclass NeuralNet:\n    def __init__(self):\n        self.alpha = 0.00001\n        self.hidden_size = 1000\n        self.weights_0_1 = np.random.random((28 * 28, self.hidden_size)) * 0.0001\n        self.weights_1_2 = np.random.random((self.hidden_size, 10)) * 0.0001\n\n    def predict(self, input):\n        layer_0 = input\n        layer_1 = relu(np.dot(layer_0, self.weights_0_1))\n        layer_2 = np.dot(layer_1, self.weights_1_2)\n        return layer_2\n\n    def train(self, input, labels, epochs):\n        for i in range(epochs):\n            layer_2_error = 0\n            for j in range(len(input)):\n                layer_0 = input[j]\n                layer_1 = relu(np.dot(layer_0, self.weights_0_1))\n                layer_2 = np.dot(layer_1, self.weights_1_2)    \n                \n                label = labels[j]\n                goal = np.zeros(10)\n                goal[label] = 1\n\n                layer_2_error = np.sum((layer_2 - goal) ** 2)\n\n                layer_2_delta = (layer_2 - goal)\n                layer_1_delta = layer_2_delta.dot(self.weights_1_2.T) * relu2deriv(layer_1)\n                \n                layer_2_weight_delta = layer_1.T.dot(layer_2_delta)\n                layer_1_weight_delta = layer_0.T.dot(layer_1_delta)\n\n                self.weights_1_2 = self.weights_1_2 - (self.alpha * layer_2_weight_delta)\n                self.weights_0_1 = self.weights_0_1 - (self.alpha * layer_1_weight_delta)\n            \n            print(\"Error: \" + str(layer_2_error))\n\n\nprepared_images = [flatten_image(image) for image in images]\nprepared_labels = np.array(labels)\n\nnn = NeuralNet()\nnn.train(prepared_images, prepared_labels, 5)\n\ntest_set = x_test\ntest_labels = y_test\nnum_correct = 0\nfor i in range(len(test_set)):\n    prediction = nn.predict(flatten_image(test_set[i]))\n    correct = test_labels[i]\n    if np.argmax(prediction) == int(correct):\n        num_correct += 1\n\nprint(str(num_correct/len(test_set) * 100) + \"%\")\n\nError: 0.10415136838556452\nError: 0.07391762691913144\nError: 0.0634422993538635\nError: 0.05133955942375137\nError: 0.039768839003841615\n97.48%\n\n\nIf I train this network over 5 epochs like the other networks, I see the error go down to 0.03 and I get 97.48% accuracy.\nNot too shabby! And what an improvement over the 1 layer network which only got 76% correct!\n\n\nWhat’s Next?\nNext I’ll dive into regularization and try to increase the accuracy even further.\nSee you then!\nFind me on Twitter if you want discuss any of what I’ve written!"
  },
  {
    "objectID": "posts/2022-02-07-how-i-implemented-speed-run-ethereum-challenge-1-decentralized-staking-app.html",
    "href": "posts/2022-02-07-how-i-implemented-speed-run-ethereum-challenge-1-decentralized-staking-app.html",
    "title": "How I Implemented Speed Run Ethereum Challenge 1: Decentralized Staking App",
    "section": "",
    "text": "In this post, I’ll show you how I implemented Challenge 1 of Speed Run ETH.\nHere’s the description from the challenge page: https://speedrunethereum.com/challenge/decentralized-staking\n\n🏦 Build a Staker.sol contract that collects ETH from numerous addresses using a payable stake() function and keeps track of balances. After some deadline if it has at least some threshold of ETH, it sends it to an ExampleExternalContract and triggers the complete() action sending the full balance. If not enough ETH is collected, allow users to withdraw()."
  },
  {
    "objectID": "posts/2022-02-07-how-i-implemented-speed-run-ethereum-challenge-1-decentralized-staking-app.html#extra-credit---modifiers",
    "href": "posts/2022-02-07-how-i-implemented-speed-run-ethereum-challenge-1-decentralized-staking-app.html#extra-credit---modifiers",
    "title": "How I Implemented Speed Run Ethereum Challenge 1: Decentralized Staking App",
    "section": "Extra Credit - modifiers",
    "text": "Extra Credit - modifiers\nI’ve included 2 modifiers here that are useful for later challenges. I separated these specific validations because they’re needed in multiple functions as we’ll see later.\nThe first is deadlinePassed. If I pass false to this modifer, it means the function will only execute if the deadline has NOT passed. We’ll see a different usage of this modifier in the withdraw function.\nThere is also the stakingNotCompleted modifier. If the balance has already been sent to the external contract, staking is completed and execute will fail and revert."
  },
  {
    "objectID": "posts/2022-05-31-how-i-implemented-speed-run-ethereum-challenge-2-token-vendor.html",
    "href": "posts/2022-05-31-how-i-implemented-speed-run-ethereum-challenge-2-token-vendor.html",
    "title": "How I Implemented Speed Run Ethereum Challenge 2: Token Vendor",
    "section": "",
    "text": "In this post, I’ll show you how I implemented Challenge 2 of Speed Run Ethereum\n\n\n\n🏵 Create YourToken.sol smart contract that inherits the ERC20 token standard from OpenZeppelin. Set your token to _mint() 1000 (* 10 ** 18) tokens to the msg.sender. Then create a Vendor.sol contract that sells your token using a payable buyTokens() function.\n\nSpeed Run Ethereum"
  },
  {
    "objectID": "posts/2022-05-31-how-i-implemented-speed-run-ethereum-challenge-2-token-vendor.html#challenge-2-token-vendor",
    "href": "posts/2022-05-31-how-i-implemented-speed-run-ethereum-challenge-2-token-vendor.html#challenge-2-token-vendor",
    "title": "How I Implemented Speed Run Ethereum Challenge 2: Token Vendor",
    "section": "",
    "text": "🏵 Create YourToken.sol smart contract that inherits the ERC20 token standard from OpenZeppelin. Set your token to _mint() 1000 (* 10 ** 18) tokens to the msg.sender. Then create a Vendor.sol contract that sells your token using a payable buyTokens() function.\n\nSpeed Run Ethereum"
  },
  {
    "objectID": "posts/2022-05-31-how-i-implemented-speed-run-ethereum-challenge-2-token-vendor.html#the-approve-pattern",
    "href": "posts/2022-05-31-how-i-implemented-speed-run-ethereum-challenge-2-token-vendor.html#the-approve-pattern",
    "title": "How I Implemented Speed Run Ethereum Challenge 2: Token Vendor",
    "section": "The Approve Pattern",
    "text": "The Approve Pattern\nIn order for a contract to move tokens in your wallet, you need to approve the contract to do so. This means that there needs to be 2 transactions in order for the vendor to buy your tokens back.\n\nYour wallet approves the vendor contract and an associated number of tokens.\nYour wallet sends the transaction to sell tokens back.\n\nThis is why using sites like Uniswap is a 2 step process. You first need to approve Uniswap to move the tokens for you and then send another transaction to actually swap the tokens.\nThe ERC20 token contract provided by OpenZeppelin comes with an Approve function. Update the UI to expose the button to approve. After the user approves, we can move the tokens out of their wallet back to the vendor. Then the vendor sends ETH to the caller.\n  /// Allow users to sell tokens back to the vendor\n  function sellTokens(uint256 amount) public {\n    // Validate token amount\n    require(amount &gt; 0, \"Must sell a token amount greater than 0\");\n\n    // Validate the user has the tokens to sell\n    address user = msg.sender;\n    uint256 userBalance = yourToken.balanceOf(user);\n    require(userBalance &gt;= amount, \"User does not have enough tokens\");\n\n    // Validate the vendor has enough ETH\n    uint256 amountOfEth = amount / tokensPerEth;\n    uint256 vendorEthBalance = address(this).balance;\n    require(vendorEthBalance &gt; amountOfEth, \"Vendor does not have enough ETH\");\n\n    // Transfer tokens\n    (bool sent) = yourToken.transferFrom(user, address(this), amount);\n    require(sent, \"Failed to transfer tokens\");\n\n    // Transfer ETH\n    (bool ethSent, ) = user.call{value: amountOfEth }(\"\");\n    require(ethSent, \"Failed to send back eth\");\n\n    // Emit sell event\n    emit SellTokens(user, amountOfEth, amount);\n  }"
  },
  {
    "objectID": "posts/2021-03-10-mnist-numpy.html",
    "href": "posts/2021-03-10-mnist-numpy.html",
    "title": "How I Go From 70 Lines Of Code To Only 26 Using The NumPy Library",
    "section": "",
    "text": "Context\nIn my previous post I implemented a nerual network to understand handwritten digits using matrix math functions I implemented myself.\nLuckily, I don’t have to be doing this going forward in the future. The NumPy library will do this all for me. Numpy is the one of the foundational libraries in the Python scientific computing ecosystem. It provides a high-performance, multidimentional array object which makes it fast and easy to work with matrices.\nIn this post, I’ll show you how I use NumPy to replace the hand written math functions I wrote.\n\n\nThe Original Code Using Only Python\nBelow is the original code with my owm matrix multiplication functions.\n\ndef flatten_image(image):\n    return list(itertools.chain.from_iterable(image))\n\ndef weighted_sum(a, b):\n    assert(len(a) == len(b))\n    output = 0\n    for i in range(len(a)):\n        output += (a[i] * b[i])\n    return output\n\ndef vector_matrix_multiplication(a, b):\n    output = [0 for i in range(10)]\n    for i in range(len(output)):\n        assert(len(a) == len(b[i]))\n        output[i] = weighted_sum(a, b[i])\n    return output\n\ndef zeros_matrix(rows, cols):\n    output = []\n    for r in range(rows):\n        output.append([0 for col in range(cols)])\n    return output\n\ndef outer_product(a, b):\n    output = zeros_matrix(len(a), len(b))\n    for i in range(len(a)):\n        for j in range(len(b)):\n            output[i][j] = a[i] * b[j]\n    return output\n\nclass NeuralNet:\n    def __init__(self):\n        self.weights = [\n            [0.0000 for i in range(784)],\n            [0.0001 for i in range(784)],\n            [0.0002 for i in range(784)],\n            [0.0003 for i in range(784)],\n            [0.0004 for i in range(784)],\n            [0.0005 for i in range(784)],\n            [0.0006 for i in range(784)],\n            [0.0007 for i in range(784)],\n            [0.0008 for i in range(784)],\n            [0.0009 for i in range(784)]\n        ]\n        self.alpha = 0.0000001\n    \n    def predict(self, input):\n        return vector_matrix_multiplication(input, self.weights)\n    \n    def train(self, input, labels, epochs):\n        for i in range(epochs):\n            for j in range(len(input)):\n                pred = self.predict(input[j])\n                \n                label = labels[j]\n                goal = [0 for k in range(10)]\n                goal[label] = 1\n                                \n                error = [0 for k in range(10)]\n                delta = [0 for k in range(10)]\n\n                for a in range(len(goal)):\n                    delta[a] = pred[a] - goal[a]\n                    error[a] = delta[a] ** 2\n                \n                weight_deltas = outer_product(delta, input[j])\n\n                for x in range(len(self.weights)):\n                    for y in range(len(self.weights[0])):\n                        self.weights[x][y] -= (self.alpha * weight_deltas[x][y])\n\n\n\nImplement The Helper Functions With NumPy\nTo help me better understand how NumPy slots into this code, I’m going to keep my helper functions but implement them using NumPy. For example, I still have a weighted sum function but instead of hand calculating the weighted sum, I use the NumPy dot function.\n\ndef flatten_image(image):\n    return image.reshape(1, 28*28)\n    \ndef weighted_sum(a, b):\n    return a.dot(b)\n\ndef vector_matrix_multiplication(a, b):\n    return np.matmul(input, weights.T)\n\ndef zeros_matrix(rows, cols):\n    return np.zeros((rows, cols))\n\ndef outer_product(a, b):\n    return np.outer(a, b)\n\nclass NeuralNet:\n    def __init__(self):\n        self.weights = np.random.random((10, 28 * 28)) * 0.0001\n        self.alpha = 0.0000001\n    \n    def predict(self, input):\n        return vector_matrix_multiplication(input, self.weights)\n    \n    def train(self, input, labels, epochs):\n        for i in range(epochs):\n            for j in range(len(input)):\n                pred = self.predict(input[j])\n                \n                label = labels[j]\n                goal = np.zeros(10)\n                goal[label] = 1                       \n               \n                delta = pred - goal\n                error = delta ** 2\n                \n                weight_deltas = outer_product(delta, input[j])\n\n                self.weights -= (self.alpha * weight_deltas)\n\n\n\nUse The NumPy Functions Inline\nAlready you can see the code is a lot cleaner.\nIf we remove those helper functions and do everything inline, the code shrinks even more. The original implmentation is 70 lines long and this one is only 26 lines.\nThe NumPy library is doing a lot of work for me.\n\ndef flatten_image(image):\n    return image.reshape(1, 28*28)\n\nclass NeuralNet:\n    def __init__(self):\n        self.weights = np.random.random((10, 28 * 28)) * 0.0001\n        self.alpha = 0.0000001\n\n    def predict(self, input):\n        return np.matmul(input, self.weights.T)\n\n    def train(self, input, labels, epochs):\n        for i in range(epochs):\n            for j in range(len(input)):\n                pred = self.predict(input[j])\n\n                label = labels[j]\n                goal = np.zeros(10)\n                goal[label] = 1\n\n                delta = pred - goal\n                error = delta ** 2\n\n                weight_deltas = np.outer(delta, input[j])\n\n                self.weights -= (self.alpha * weight_deltas)\n\n\nimport itertools\nimport numpy as np\n\nfrom keras.datasets import mnist\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nimages = x_train\nlabels = y_train\n\nprepared_images = [flatten_image(image) for image in images]\nprepared_labels = np.array(labels)\n\nnn = NeuralNet()\nnn.train(prepared_images, prepared_labels, 5)\n\ntest_set = x_test\ntest_labels = y_test\nnum_correct = 0\nfor i in range(len(test_set)):\n    prediction = nn.predict(flatten_image(test_set[i]))\n    correct = test_labels[i]\n    if np.argmax(prediction) == int(correct):\n        num_correct += 1\n\nprint(str(num_correct/len(test_set) * 100) + \"%\")\n\n76.05%\n\n\n\n\nSo What’s Next?\nThe code is now so efficient I can train on the full dataset and the accuracy gets a little bump.\nIn the next post we’ll see if adding a layer helps improve this accuracy even more."
  },
  {
    "objectID": "posts/2021-01-22-how-i-implemented-the-most-simple-neural-net.html",
    "href": "posts/2021-01-22-how-i-implemented-the-most-simple-neural-net.html",
    "title": "How I Implemented The Most Simple Neural Network Using Python",
    "section": "",
    "text": "I’ve been reading the book Grokking Deep Learning by Andrew W. Trask and instead of summarizing concepts, I want to review them by building a simple neural network. This neural network will use the concepts in the first 4 chapters of the book."
  },
  {
    "objectID": "posts/2021-01-22-how-i-implemented-the-most-simple-neural-net.html#the-prediction",
    "href": "posts/2021-01-22-how-i-implemented-the-most-simple-neural-net.html#the-prediction",
    "title": "How I Implemented The Most Simple Neural Network Using Python",
    "section": "1. The Prediction",
    "text": "1. The Prediction\npred = input * self.weight\nWhen the neural network has both an input and weight, it multiplies them together to make a prediction. Every single neural network, from the most simple to ones with 1000s of layers works this way."
  },
  {
    "objectID": "posts/2021-01-22-how-i-implemented-the-most-simple-neural-net.html#how-much-are-we-off-by",
    "href": "posts/2021-01-22-how-i-implemented-the-most-simple-neural-net.html#how-much-are-we-off-by",
    "title": "How I Implemented The Most Simple Neural Network Using Python",
    "section": "2. How much are we off by?",
    "text": "2. How much are we off by?\ndelta = pred - goal\nerror = delta ** 2\nSo we’ve seen that the network make a prediction by multiplying input and weight. After it makes a prediction, the network is able to calculate how much it was off by.\nA neural network learning is all about error attribution. How much did each weight contribute to the overall error of the system and how can we change the weight so that error is minimized? In our example, it’s easy to figure out since there is only 1 weight.\nHow do we calculate the error? One thing we need to keep in mind is that we want the error to be a positive number. If the error is allowed to be negative, multiple errors might accidentally cancel each other out when averaged together.\nIn our case, we square the amount we are off by. Why square instead of something straightforward like absolute value? Squaring gives us a sense of importance. Large errors are magnified while small errors are minimized. Therefore, we can prioritize large errors before small errors. Absolute value doesn’t give us this additional sense of importance."
  },
  {
    "objectID": "posts/2021-01-22-how-i-implemented-the-most-simple-neural-net.html#adjusting-the-weights",
    "href": "posts/2021-01-22-how-i-implemented-the-most-simple-neural-net.html#adjusting-the-weights",
    "title": "How I Implemented The Most Simple Neural Network Using Python",
    "section": "3. Adjusting the weights",
    "text": "3. Adjusting the weights\nderivative = delta * input\nself.weight = self.weight - (self.alpha * derivative)\nThe network figures out how much to adjust the weights by using a derivative. How does derivative play into this process? What a derivative tells us is the direction and amount one variable changes when you change a different variable. In our case, derivatives tell us much much error changes when you change the weight. Given that we want error to be 0, this is exactly what we need.\nThe network calculates the derivative by multiplying the delta by the weight’s input to get the weight_delta. weight_delta is the direction and the amount we’re going to change the weight by.\nself.alpha = 0.01\nOne bit of nuance is the variable alpha. alpha is a throttle limiting how much we actually adjust the weights. Determining the appropriate rate of change for the weights of a neural network is a challenge. If the steps are too large, the network will overshoot the error getting to zero and start acting in unpredictable ways. If the steps are too small, the network will take a long time and need a very large number of training cycles.\nThe solution to this problem is to multiply partial derivative by a single number between 0 and 1. This lets us control the rate of change and adjust the learning as needed.\nFinding the appropriate alpha is often done through trial and error so we’re just going to hard code is here."
  },
  {
    "objectID": "posts/2021-01-22-how-i-implemented-the-most-simple-neural-net.html#training-rounds",
    "href": "posts/2021-01-22-how-i-implemented-the-most-simple-neural-net.html#training-rounds",
    "title": "How I Implemented The Most Simple Neural Network Using Python",
    "section": "4. Training rounds",
    "text": "4. Training rounds\nneural_network.train(input=5, goal=42, epochs=20)\n\nfor i in range(epochs):\nFinally, there’s the concept of epochs. This refers to the number of times the network will go through the entire data set. The appropriate number of epochs for a problem will often be found through trial and error.\nI’m using 20 in the example, which I found by running the training with different epochs and picking the lowest one with an acceptable error. Feel free to experiment with the number of epochs and see what happens at different numbers."
  },
  {
    "objectID": "posts/2021-02-28-mnist.html",
    "href": "posts/2021-02-28-mnist.html",
    "title": "How I Identify Handwritten Digits Using Only Python",
    "section": "",
    "text": "What I’m Building\nIn this post I’ll show you how I built a neural network which takes an array of numbers representing a handwritten digit and output a prediction of what digit it is.\nThe handwritten digits are from the famous MNIST dataset. The Modified National Institute of Standards and Technology (MNIST) dataset is a collection of 60,000 small, square 28×28 pixel grayscale images of handwritten single digits between 0 and 9.\nThe task is to classify a given image into one of the 10 digits.\nI’m doing it all in Python.\nLet’s get started.\n\n\nThe Code\n\nimport itertools\n\nfrom keras.datasets import mnist\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nimages = x_train[0:1000]\nlabels = y_train[0:1000]\n\ndef flatten_image(image):\n    return list(itertools.chain.from_iterable(image))\n\ndef weighted_sum(a, b):\n    assert(len(a) == len(b))\n    output = 0\n    for i in range(len(a)):\n        output += (a[i] * b[i])\n    return output\n\ndef vector_matrix_multiplication(a, b):\n    output = [0 for i in range(10)]\n    for i in range(len(output)):\n        assert(len(a) == len(b[i]))\n        output[i] = weighted_sum(a, b[i])\n    return output\n\ndef zeros_matrix(rows, cols):\n    output = []\n    for r in range(rows):\n        output.append([0 for col in range(cols)])\n    return output\n\ndef outer_product(a, b):\n    output = zeros_matrix(len(a), len(b))\n    for i in range(len(a)):\n        for j in range(len(b)):\n            output[i][j] = a[i] * b[j]\n    return output\n\nclass NeuralNet:\n    def __init__(self):\n        self.weights = [\n            [0.0000 for i in range(784)],\n            [0.0001 for i in range(784)],\n            [0.0002 for i in range(784)],\n            [0.0003 for i in range(784)],\n            [0.0004 for i in range(784)],\n            [0.0005 for i in range(784)],\n            [0.0006 for i in range(784)],\n            [0.0007 for i in range(784)],\n            [0.0008 for i in range(784)],\n            [0.0009 for i in range(784)]\n        ]\n        self.alpha = 0.0000001\n    \n    def predict(self, input):\n        return vector_matrix_multiplication(input, self.weights)\n    \n    def train(self, input, labels, epochs):\n        for i in range(epochs):\n            for j in range(len(input)):\n                pred = self.predict(input[j])\n                \n                label = labels[j]\n                goal = [0 for k in range(10)]\n                goal[label] = 1\n                                \n                error = [0 for k in range(10)]\n                delta = [0 for k in range(10)]\n\n                for a in range(len(goal)):\n                    delta[a] = pred[a] - goal[a]\n                    error[a] = delta[a] ** 2\n                \n                weight_deltas = outer_product(delta, input[j])\n\n                for x in range(len(self.weights)):\n                    for y in range(len(self.weights[0])):\n                        self.weights[x][y] -= (self.alpha * weight_deltas[x][y])\n\n# Train on first image\nfirst_image = images[0]\nfirst_label = labels[0]\ninput = [flatten_image(first_image)]\nlabel = [first_label]\n\nnn = NeuralNet()\nnn.train(input, label, 5)\n\nprediction = nn.predict(input[0])\nprint(prediction)\nprint(\"The label is: \" + str(label[0]) + \". The prediction is: \" + str(prediction.index(max(prediction))))\n\n# Train on full dataset\nprepared_images = [flatten_image(image) for image in images]\nmm = NeuralNet()\nmm.train(prepared_images, labels, 45)\n\n# Test 1 prediction\nprediction = mm.predict(prepared_images[3])\nprint(\"That image is the number \" + str(prediction.index(max(prediction))))\n\n# Calculate accuracy\ntest_set = x_test[0:100]\ntest_labels = y_test[0:100]\nnum_correct = 0\nfor i in range(len(test_set)):\n    prediction = mm.predict(flatten_image(test_set[i]))\n    correct = test_labels[i]\n    \n    if prediction.index(max(prediction)) == int(correct):\n        num_correct += 1\n\nprint(str(num_correct/len(test_set) * 100) + \"%\")\n\n\n\nGet Dataset\nThe keras library helpfully includes the dataset so I can import it from the library.\n\nfrom keras.datasets import mnist\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nimages = x_train[0:1000]\nlabels = y_train[0:1000]\n\nWhen I call load_data(), I get back two tuples: a training set and a test set. To successfully finishing training on my personal laptop, I had to limit the data to the first 1000 elements. When I tried training on the full data set, it was hadn’t finished after a full 24 hours and I had to kill the process to use my laptop :D.\nWith only 1000 images, the best accuracy I achieved was about 75%. Maybe you can tweak the numbers and get something better!\nGetting back to the data, if I take a look at one of the images in the training set, I see that it is an array of arrays - a matrix. The numbers range from 0 to 255 - each representing the greyscale value of the pixel at a particular position in the image.\n\nimages[0]\n\narray([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0]], dtype=uint8)\n\n\nIf I look at the first label, I see the number five. This means that the collection of numbers in images[0] represents is the number 5.\n\nlabels[0]\n\n5\n\n\n\n\nPrepare Data\nThe matrix math that I implement does not know how to handle an array of arrays so, the first thing I do is prepare the data by flattening the image into a single array.\n\nimport itertools\n\ndef flatten_image(image):\n    return list(itertools.chain.from_iterable(image))\n\nWhat I’m doing in this function is using the itertools library to flatten the array. Specifically, I’m using the .chain.from_iterable() method to give me one element at a time. Then I use the list() function to create a flat list to return.\nWhen I print the first image, I see that all the numbers are in one flat array.\n\nprint(flatten_image(images[0]))\n\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 18, 18, 18, 126, 136, 175, 26, 166, 255, 247, 127, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 30, 36, 94, 154, 170, 253, 253, 253, 253, 253, 225, 172, 253, 242, 195, 64, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 49, 238, 253, 253, 253, 253, 253, 253, 253, 253, 251, 93, 82, 82, 56, 39, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 18, 219, 253, 253, 253, 253, 253, 198, 182, 247, 241, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 80, 156, 107, 253, 253, 205, 11, 0, 43, 154, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 14, 1, 154, 253, 90, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 139, 253, 190, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 190, 253, 70, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 35, 241, 225, 160, 108, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 81, 240, 253, 253, 119, 25, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 45, 186, 253, 253, 150, 27, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 93, 252, 253, 187, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 249, 253, 249, 64, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 46, 130, 183, 253, 253, 207, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 39, 148, 229, 253, 253, 253, 250, 182, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 24, 114, 221, 253, 253, 253, 253, 201, 78, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 23, 66, 213, 253, 253, 253, 253, 198, 81, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 18, 171, 219, 253, 253, 253, 253, 195, 80, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 55, 172, 226, 253, 253, 253, 253, 244, 133, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 136, 253, 253, 253, 212, 135, 132, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n\n\n\nMatrix Math Helper Functions\nNow that I’ve prepared the data, I can move on to the next step - implement matrix math.\nSince I’m working with arrays, I need math functions which understand arrays. You may remember from the previous post that a neural network makes predictions by multiplying the input by the weights. So one thing I need to do now is figure out how to do matrix multiplication.\nIn order to do matrix multipliation, I need a method to calculate weighted sums.\n\ndef weighted_sum(a, b):\n    assert(len(a) == len(b))\n    output = 0\n    for i in range(len(a)):\n        output += (a[i] * b[i])\n    return output\n\nThe weighted sum function takes two arrays of the same length. It multiplies each number in the same index and adds the result to a running sum. So the weighted sum takes two arrays and gives you back a single number.\nThe best way to think about what this single number represents is as a score of similarity between two arrays. The higher the weighted sum, the more similar arrays a and b are to each other. Roughly speaking, the neural network will give higher scores to inputs that are more similar to its weights.\n\ndef vector_matrix_multiplication(a, b):\n    output = [0 for i in range(10)]\n    for i in range(len(output)):\n        assert(len(a) == len(b[i]))\n        output[i] = weighted_sum(a, b[i])\n    return output\n\nNext, I have the matrix multiplication method. This calculates the weighted sum between weight and input for each position in the array. When it’s done, I get an array of weighted sums.\nIn my case, the returned output of 10 elements contain the probability of which digit the input represents. Whichever index has the highest number is the prediction for what digit is in the image.\nI need two other matrix math helpers. These functions will be used to adjust the weights in the right direction.\nFirst, I have a zeros matrix method which creates a matrix filled with zeros.\n\ndef zeros_matrix(rows, cols):\n    output = []\n    for r in range(rows):\n        output.append([0 for col in range(cols)])\n    return output\n\nThis is used to implement a function to calculate the outer product of two matrices.\nThe outer product does an elementwise multiplication between two matricies. This will be used to tell the neural network how to change its weights.\n\ndef outer_product(a, b):\n    output = zeros_matrix(len(a), len(b))\n    for i in range(len(a)):\n        for j in range(len(b)):\n            output[i][j] = a[i] * b[j]\n    return output\n\nOkay. That’s a lot of math. Let’s find out how these functions are being used in the neural network.\n\n\nNeural Network\n\nclass NeuralNet:\n    def __init__(self):\n        self.weights = [\n            [0.0000 for i in range(784)],\n            [0.0001 for i in range(784)],\n            [0.0002 for i in range(784)],\n            [0.0003 for i in range(784)],\n            [0.0004 for i in range(784)],\n            [0.0005 for i in range(784)],\n            [0.0006 for i in range(784)],\n            [0.0007 for i in range(784)],\n            [0.0008 for i in range(784)],\n            [0.0009 for i in range(784)]\n        ]\n        self.alpha = 0.0000001\n    \n    def predict(self, input):\n        return vector_matrix_multiplication(input, self.weights)\n    \n    def train(self, input, labels, epochs):\n        for i in range(epochs):\n            for j in range(len(input)):\n                pred = self.predict(input[j])\n                \n                label = labels[j]\n                goal = [0 for k in range(10)]\n                goal[label] = 1\n                                \n                error = [0 for k in range(10)]\n                delta = [0 for k in range(10)]\n\n                for a in range(len(goal)):\n                    delta[a] = pred[a] - goal[a]\n                    error[a] = delta[a] ** 2\n                \n                weight_deltas = outer_product(delta, input[j])\n\n                for x in range(len(self.weights)):\n                    for y in range(len(self.weights[0])):\n                        self.weights[x][y] -= (self.alpha * weight_deltas[x][y])\n    \n\nThis neural network is similar to the one from the previous post. The only real difference is that we’re using an array of numbers instead of a single number.\nIn the initializer, I have the weights and the alpha. I’ve initialized each weight array to have 784 elements of an initial number. 784 is the number of pixels in the image.\ndef __init__(self):\n    self.weights = [\n        [0.0000 for i in range(784)],\n        [0.0001 for i in range(784)],\n        [0.0002 for i in range(784)],\n        [0.0003 for i in range(784)],\n        [0.0004 for i in range(784)],\n        [0.0005 for i in range(784)],\n        [0.0006 for i in range(784)],\n        [0.0007 for i in range(784)],\n        [0.0008 for i in range(784)],\n        [0.0009 for i in range(784)]\n    ]\n    self.alpha = 0.0000001\nThe prediction function is again multplying the input by the weights.\ndef predict(self, input):\n    return vector_matrix_multiplication(input, self.weights)\nThe training function iterates through the dataset an epoch number of times.\nfor i in range(epochs):\n    for j in range(len(input)):\nFor each image, it makes a prediction\npred = self.predict(input[j])\nNext we transform the label into a format that the neural network expects.\nlabel = labels[j]\ngoal = [0 for k in range(10)]\ngoal[label] = 1\nI create an array of ten 0s and then set the index of the goal prediction to 1. So all the wrong answers are 0 and the right answer is 1.\nNext, I calculate the error and the delta.\nerror = [0 for k in range(10)]\ndelta = [0 for k in range(10)]\n\nfor a in range(len(goal)):\n    delta[a] = pred[a] - goal[a]\n    error[a] = delta[a] ** 2\nI then calculate the weight deltas by using an outer product between delta and the input.\nweight_deltas = outer_product(delta, input[j])\nFinally I update all the weights using the weight deltas.\nfor x in range(len(self.weights)):\n    for y in range(len(self.weights[0])):\n        self.weights[x][y] -= (self.alpha * weight_deltas[x][y])\nThe main takeaway here is that this is exactly like the neural network with one digit. The only difference is that the math is done on arrays instead of on single numbers.\n\n\nTraining The Network On The First Data Point\nLet’s put this new network into action. To test it out, I take take the first image and the first label. I create a neural network and train it on that first image and label for five epochs. When I predict the digit on that same image, I see the output array is an array of 10 numbers.\n\nfirst_image = images[0]\nfirst_label = labels[0]\ninput = [flatten_image(first_image)]\nlabel = [first_label]\n\nnn = NeuralNet()\nnn.train(input, label, 5)\n\nprediction = nn.predict(input[0])\nprint(prediction)\nprint(\"The label is: \" + str(label[0]) + \". The prediction is: \" + str(prediction.index(max(prediction))))\n\n[0.0, 0.03036370905054081, 0.06072741810108162, 0.09109112715162263, 0.12145483620216324, 1.1407872249800253, 0.18218225430324525, 0.21254596335378556, 0.24290967240432648, 0.2732733814548679]\nThe label is: 5. The prediction is: 5\n\n\nThe number in index five is the greatest, so the network correctly identified the handwritten number of the number 5.\nIt works on one data point but what about the entire data set?\nLet’s do that next.\n\n\nTraining The Network On All The Whole Dataset\nI prepare the images by flattening every image in our data set. Again, this is the first 1000 from the MNIST dataset. I create the neural network, giving it the prepared images and labels.\nI run it for 5 epochs. Through trial and error I found that 5 epochs gives me the highest accuracy of just under 75%.\nWhen it’s finished, I test the network by making a prediction on a random image. It correctly identified the image.\n\nprepared_images = [flatten_image(image) for image in images]\n\nmm = NeuralNet()\nmm.train(prepared_images, labels, 5)\n\nprediction = mm.predict(prepared_images[3])\nprint(\"That image is the number \" + str(prediction.index(max(prediction))))\n\nThat image is the number 1\n\n\n\nlabels[3]\n\n1\n\n\nTo test the true accuracy, I use the test data and labels.\nI run through a loop of the test set, make a prediction, checking its accuracy, and counting the number correct.\n\ntest_set = x_test\ntest_labels = y_test\nnum_correct = 0\nfor i in range(len(test_set)):\n    prediction = mm.predict(flatten_image(test_set[i]))\n    correct = test_labels[i]\n    if prediction.index(max(prediction)) == int(correct):\n        num_correct += 1\n\nprint(str(num_correct/len(test_set) * 100) + \"%\")\n\n74.47%\n\n\nIn the end, I’m able to correctly predict 3 out of every 4 images in the test set.\n\n\nSo What Did We Do?\nThis was a fun little exercise to see how neural networks use matrix math to make predictions.\n\n\nWhat’s Next?\nIn the next post, I’ll experiment with adding multiple layers to make the network “deep”. I’ll also swap my handwritten matrix math functions for NumPy functions and see how much easier it makes some of this for me.\nSee you next time!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "leogau.dev",
    "section": "",
    "text": "How I Implemented Speed Run Ethereum Challenge 2: Token Vendor\n\n\n\n\n\n\nsolidity\n\n\nethereum\n\n\nweb3\n\n\ncryptocurrency\n\n\n\nExplaining my accepted solution to Speed Run Ethereum Challenge 2\n\n\n\n\n\nMay 31, 2022\n\n\nLeo Gau\n\n\n\n\n\n\n\n\n\n\n\n\nHow I Implemented Speed Run Ethereum Challenge 1: Decentralized Staking App\n\n\n\n\n\n\nsolidity\n\n\nethereum\n\n\nweb3\n\n\ncryptocurrency\n\n\n\nExplaining my accepted solution to Speed Run Ethereum Challenge 1\n\n\n\n\n\nFeb 7, 2022\n\n\nLeo Gau\n\n\n\n\n\n\n\n\n\n\n\n\nIdentify MNIST Digits With 97% Accuracy By Using A 2 Layer Neural Network\n\n\n\n\n\n\nGrokking Deep Learning\n\n\nDeep Learning\n\n\nPython\n\n\n\nAdding a hidden layer increased my accuracy by over 20%\n\n\n\n\n\nMar 16, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nHow I Go From 70 Lines Of Code To Only 26 Using The NumPy Library\n\n\n\n\n\n\nGrokking Deep Learning\n\n\nDeep Learning\n\n\nPython\n\n\n\nReplacing my hand written math functions with NumPy\n\n\n\n\n\nMar 10, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nHow I Identify Handwritten Digits Using Only Python\n\n\n\n\n\n\nGrokking Deep Learning\n\n\nDeep Learning\n\n\nPython\n\n\n\nTraining a neural network to correctly identify digits from the MNIST dataset\n\n\n\n\n\nFeb 28, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nHow I Implemented The Most Simple Neural Network Using Python\n\n\n\n\n\n\nGrokking Deep Learning\n\n\nDeep Learning\n\n\nPython\n\n\n\nTraining a neural network to output ‘42’ when given ‘5’\n\n\n\n\n\nJan 22, 2021\n\n\n\n\n\n\nNo matching items"
  }
]